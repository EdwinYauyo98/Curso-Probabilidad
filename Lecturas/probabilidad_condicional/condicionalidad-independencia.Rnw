\documentclass{article}
\usepackage[sc]{mathpazo}
\usepackage[T1]{fontenc}
\usepackage{bigints}
\usepackage[latin1]{inputenc}
\usepackage{geometry}
\usepackage{amsmath} 
\usepackage{mathtools}
%\usepackage[spanish]{babel}
\usepackage{latexsym}
\geometry{verbose,tmargin=1.5cm,bmargin=2.5cm,lmargin=2.5cm,rmargin=2.5cm}
\setcounter{secnumdepth}{2}
\setcounter{tocdepth}{2}
\usepackage{url}
\usepackage{graphicx}
\newcommand{\R}{\mathbb{R}}
\newcommand{\bb}[1]{\textbf{#1}}
\DeclareMathOperator{\rank}{\textbf{rango}}
\DeclareMathOperator{\proy}{\textbf{proy}}
\DeclareMathOperator{\nulll}{\textbf{nul}}
\DeclareMathOperator{\diag}{\textbf{diag}}
\DeclareMathOperator{\col}{\textbf{col}}
\DeclareMathOperator{\fila}{\textbf{fila}}
\DeclareMathOperator{\dimm}{dim}
\DeclareMathOperator{\Traz}{Tr}
%\theoremstyle{definition}
\everymath{\displaystyle}
\newtheorem{ejemplo}{{Ejemplo }}[section]
\newtheorem{defi}{{Definici\'on}}[section]
\newtheorem{pros}{{Proposici\'on}}[section]
\newtheorem{cor}{{Corolario}}[section]
\usepackage[unicode=true,pdfusetitle,
 bookmarks=true,bookmarksnumbered=true,bookmarksopen=true,bookmarksopenlevel=2,
 breaklinks=false,pdfborder={0 0 1},backref=false,colorlinks=false]
 {hyperref}
\hypersetup{
 pdfstartview={XYZ null null 1}}
\begin{document}

<<setup, include=FALSE, cache=FALSE>>=
library(knitr)
library(ggplot2)
library(grid)
#library(animation)
# set global chunk options
opts_chunk$set(fig.path='figure/minimal-', fig.align='center', fig.show='hold')
options(formatR.arrow=TRUE,width=90)
@

%\title{\underline{\textbf{Notas de mat\'ematica}}}
%\date{}
%\maketitle
\hspace*{0.5\linewidth}
\begin{minipage}{0.6\linewidth}
Curso: C\'alculo de probabilidad CM-1H2\par
Probabilidad condicional, eventos independientes \par
\end{minipage}


\vspace {0.5cm}

\section{Probabilidad condicional}

\vspace{0.3cm}


\begin{defi}
\normalfont Asumiendo $P(B) > 0$, definimos la probabilidad condicional de $A$, dado que $B$ a ocurrido como sigue

\[
\mathbb{P}(A|B ) = \frac{\mathbb{P}(A \cap B)}{\mathbb{P}(B)}
\]

\vspace{0.3cm}

Para cualquier evento $B$ fijo  tal que  $P (B)> 0, \mathbb{P} (\cdot| B)$ es una funci\'on de  probabilidad (es decir, que cumple los tres axiomas de probabilidad). En particular $\mathbb{P}(A|B) \geq 0, \mathbb{P}(\Omega|B) = 1$ y si $A_1,A_2, \dots $ son disjuntos, entonces $\mathbb{P}(\bigcup\limits_{i= 1}^{\infty}A_i|B)= \sum_{i = 1}^{\infty}\mathbb{P}(A_i|B)$. Pero esto no es cierto en general que $\mathbb{P}(A|B \cup C) = \mathbb{P}(A|B) + P(A|C)$.

\end{defi}

\vspace{0.2cm}

Las reglas de la probabilidad se aplican a los eventos de la izquierda de la barra $|$. En general, no se cumple que $P(A|B) = P(B|A)$.

\vspace{0.2cm}

Hay mucha confusi\'on con esto todo el tiempo. Por ejemplo, la probabilidad de que tengas puntos (manchas) dado que tienes  sarampi\'on es $1$, pero la probabilidad de que tengas   sarampi\'on, dado que  tienes puntos (manchas) no es $1$. En este caso, la diferencia entre $\mathbb{P}(A|B)$  y $\mathbb{P}(B|A)$ es obvia, pero hay casos en los que es menos obvio. Este error sucede  con bastante frecuencia en los casos legales que a veces se llaman \textit{falacias del fiscal}.


\vspace{0.2cm}

Si $\mathbb{P}(A) > 0$ y $\mathbb{P}(B) > 0$, tenemos

\[
\mathbb{P}(A\cap B)=\mathbb{P}(A|B)\mathbb{P}(B)=\mathbb{P}(B|A)\mathbb{P}(A).
\]

\vspace{0.3cm}

Intuitivamente, $\mathbb{P}(A | B)$ es la probabilidad de que $A$ ocurra suponiendo que ocurri\'o el evento $B$. De acuerdo con esa intuici\'on, la probabilidad condicional tiene las siguientes propiedades

\begin{itemize}
\item Si $B \subset A$ entonces $\mathbb{P}(A| B)=\mathbb{P}(B)/\mathbb{P}(B)=1$.
\item Si $A \cap B = \emptyset$ entonces $\mathbb{P}(A | B)=0/\mathbb{P}(B)=0$.
\item $A \subset B$ entonces $\mathbb{P}(A| B)=\mathbb{P}(A)/\mathbb{P}(B)$.
\item La probabilidad condicional, puede ser vista como una funci\'on de probabilidad

\[
\mathbb{P}_A(E)  =  \mathbb{P}(E | A)
\]
y todas las propiedades y intuiciones que se aplican a la funci\'on probabilidad se aplican a $\mathbb{P}_A$ tambi\'en.

\item Asumiendo que el evento $A$ ocurri\'o, $\mathbb{P}_A$ generalmente tiene mejores habilidades de pron\'ostico que $\mathbb{P}$.
\end{itemize}


\vspace{0.2cm}

Como se mencion\'o anteriormente, las probabilidades condicionales son usualmente intuitivas. El siguiente ejemplo de (Feller, 1968), sin embargo, muestra una situaci\'on contra-intuitiva que implica probabilidades condicionales. Esto demuestra que la intuici\'on no debe ser un sustituto de la computaci\'on rigurosa.


\vspace{0.2cm}

\begin{ejemplo}
\normalfont Consideramos dos familias con dos hijos donde la probabilidad de g\'enero de cada ni\~no es sim\'etrica $(1/2)$. Seleccionamos una familia al azar y consideramos el espacio muestral que describe el g\'enero de los ni\~nos $\Omega=\{MM,MF,FM,FF\}$. Asumimos un modelo cl\'asico, implicando que las probabilidades de  todos los eventos elementales son $1/4$.

\vspace{0.2cm}

Definimos el evento  en que ambos hijos en la familia son ni\~nos como $A=\{MM\}$, el evento que una familia tiene un ni\~no es $B=\{MF,FM,MM\}$ y el evento que el primer hijo es un ni\~no como $C=\{MF,MM\}$.

\vspace{0.2cm}

Dado que el primer hijo  es un ni\~no, la probabilidad de que ambos hijos sean ni\~nos es

\[
\mathbb{P}(A | C)=\mathbb{P}(A\cap C)/\mathbb{P}(C)=\mathbb{P}(A)/\mathbb{P}(C)=(1/4)/(1/2)=1/2.
\]

\vspace{0.2cm}

Esto coincide con nuestra intuici\'on. Dado que la familia tiene un ni\~no, la probabilidad de que ambos hijos sean ni\~nos es contraintuitiva

\[
\mathbb{P}(A| B)=\mathbb{P}(A\cap B)/\mathbb{P}(B)=\mathbb{P}(A)/\mathbb{P}(B)=(1/4)/(3/4)=1/3.
\]
\end{ejemplo}


\vspace{0.2cm}


\begin{ejemplo}
\normalfont Observamos en la siguiente figura, que $\mathbb{P}(A \cap B) = 1/6$, que $\mathbb{P}(B) = 1/2$ y que $\mathbb{P}(A|B) = (1/6)/(1/2) = 1/3$, como se esperaba. Sabemos que $B$ ha ocurrido y que el evento $A$ ocurrir\'a si uno de los cuatro resultados en $A \cap B$ se elige entre los $12$ igualmente probable.

\begin{figure}[h]
\centering
\includegraphics[width=6cm]{g1}
\end{figure}
\end{ejemplo}

\vspace{0.5cm}

Podemos generalizar la ecuaci\'on de la probabilidad condicional a m\'ultiples eventos. Sea $A_i, i = 1,2, \dots, k$ para $k$ eventos para el cual $\mathbb{P}(A_1 \cap A_2 \cap \cdots A_k) > 0$. Entonces

\begin{align*}
\mathbb{P}(A_1 \cap A_2 \cap \cdots A_k) = \mathbb{P}(A_1)\mathbb{P}(A_2|A_1)\mathbb{P_3}(A_3|A_1 \cap A_2)\cdots \mathbb{P}(A_k|A_1 \cap A_2 \cap \cdots \cap A_{k -1}).
\end{align*}


\vspace{0.2cm}

\begin{ejemplo}
\normalfont En una clase de nivel de posgrado de primer a\~no de 60 estudiantes, diez estudiantes son estudiantes de pregrado.

Calculemos la probabilidad de que tres estudiantes elegidos al azar sean estudiantes de pregrado. Para ello sea $A_1$ el evento en el cual el primer estudiante elegido es un estudiante de pregrado, $A_2$ el evento en que el segundo  estudiante sea de pregrado y as\'i sucesivamente. Luego de la ecuaci\'on anterior tenemos

\[
\mathbb{P}(A_1 \cap A_2 \cap A_3) = \mathbb{P}(A_1)\mathbb{P}(A_2|A_1)\mathbb{P}(A_3|A_1 \cap A_2)
\]

reemplazando obtenemos 

\[
\mathbb{P}(A_1 \cap A_2 \cap A_3) = \frac{10}{60}\times \frac{9}{59} \times \frac{8}{58} = 0.003507.
\]


\end{ejemplo}

\vspace{0.5cm}

\begin{ejemplo}
\normalfont Un test m\'edico para una enfermedad $D$ tiene salidas + y -

\vspace{0.3cm}

\begin{tabular}{l|c  r}
   Salidas           & D & $D^{c}$  \\
\hline
+       & .009 & 0.099   \\
-       & .001 & 0.891   \\
\end{tabular}


\vspace{0.3cm}

Desde la definici\'on de probabilidad  condicional,

\vspace{0.2cm}

\[
\mathbb{P}(+| D) = \frac{\mathbb{P}(+ \cap D)}{\mathbb{P}(D)} = \frac{.009}{.009 + .001} = .9
\]

\vspace{0.2cm}

y

\vspace{0.2cm}

\[
\mathbb{P}(-| D^{c}) = \frac{\mathbb{P}(- \cap D^{c})}{\mathbb{P}(D^{c})} = \frac{.891}{.891 + .099} \approx .9
\]


\vspace{0.2cm}

Al parecer, la prueba es bastante exacta. Personas enfermas producen un positivo del $90$ por ciento de la veces y las personas sanas producen una negativa sobre 90 por ciento de las veces . Supongamos que una persona se  hace  una prueba y obtiene un resultado positivo. ?` Cu\'al es la probabilidad de que esa persona  tiene la enfermedad? Muchas personas responden que   $0,90$, sin embargo la  respuesta correcta es

\vspace{0.2cm}

\[
\mathbb{P}(D| +) = \frac{\mathbb{P}(+ \cap D)}{\mathbb{P}(+)} = \frac{.009}{.009 + .099} \approx .08
\]


\end{ejemplo}

\vspace{0.3cm}

\begin{ejemplo}

\normalfont Una peque\~na empresa de software emplea 4 programadores. El porcentaje de c\'odigo escrito por cada programador y el porcentaje de errores en su c\'odigo se muestran en la siguiente tabla. El c\'odigo es seleccionado al azar.

\begin{table}[h]
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
Programador & \% de c\'odigo escrito & \% de errores en c\'odigo \\ \hline
1           & 15                   & 5                       \\ \hline
2           & 20                   & 3                       \\ \hline
3           & 25                   & 2                       \\ \hline
4           & 40                   & 1                       \\ \hline
\end{tabular}
\end{center}
\end{table}

\vspace{0.3cm}

\begin{enumerate}
\item Antes de que el c\'odigo sea examinado, ?` cu\'al es la probabilidad de que fue escrito por el

\begin{verbatim}
  Programador 1? .15
  Programador 2? .20
  Programador 3? .25
  Programador 4? .40
\end{verbatim}
\end{enumerate}

\vspace{0.3cm}

Aplicando  la definici\'on de probabilidad condicional tenemos

\vspace{0.2cm}


\begin{align*}
\mathbb{P}(\mbox{Prog1|no error}) = \frac{\mathbb{P}(\mbox{Prog1} \cap \mbox{no error})}{\mathbb{P}(\mbox{no error})}\\
\mathbb{P}(\mbox{Progr1} \cap \mbox{no error}) = 0.15 *0.95 = 0.1425
\end{align*}

\vspace{0.3cm}

$\mathbb{P}$(no error)


<<foo8w, comment = NA, prompt =TRUE, eval= TRUE>>=
0.15*0.95 + 0.20 *0.97 + 0.25 * 0.98 + 0.40 * 0.99
@

\vspace{0.2cm}

$\mathbb{P}(\mbox{Progr1}|\mbox{no error}) = 0.1425/0.9775 = 0.1457428$.


\vspace{0.2cm}

$\mathbb{P}(\mbox{Progr2}|\mbox{no error})$

<<foo8y, comment = NA, prompt =TRUE, eval= TRUE>>=
0.20*0.97/0.97775
@

\vspace{0.2cm}

$\mathbb{P}(\mbox{Progr3}|\mbox{no error})$

<<foo8x, comment = NA, prompt =TRUE, eval= TRUE>>=
0.25*0.98/0.97775
@

\vspace{0.2cm}

$\mathbb{P}(\mbox{Progr4} |\mbox{no error})$
<<foo8z, comment = NA, prompt =TRUE, eval= TRUE>>=
0.40*0.99/0.97775
@

\end{ejemplo}

\section{ Eventos independientes}

\vspace{0.3cm}

\begin{defi}
\normalfont Dos eventos  $A$ y $B$ son independientes si  $\mathbb{P}(A\cap B) = \mathbb{P}(A)\mathbb{P}(B)$.  Para tres eventos, $A, B$ y $C$ definimos los eventos independientes si

\begin{align*}
\mathbb{P}(A \cap B) &= \mathbb{P}(A)\mathbb{P}(B)\\
\mathbb{P}(A \cap C) &= \mathbb{P}(A) \mathbb{P}(C)\\
\mathbb{P}(B \cap C) &= \mathbb{P}(A)\mathbb{P}(C)\ \ \text{y} \\
\mathbb{P}(A \cap B \cap C) &= \mathbb{P}(A)\mathbb{P}(B)\mathbb{P}(C)
\end{align*}



\vspace{0.3cm}

Las tres primeras de estas condiciones establecen que los eventos son independientes dos a dos , por lo que llamamos a estos eventos que satisfacen estas tres condiciones como \texttt{independientes por pares}. El ejemplo  siguiente  mostrar\'a que los eventos que satisfacen estas tres condiciones pueden no satisfacer la cuarta condici\'on, de modo que la independencia por pares no determina la independencia.


\vspace{0.2cm}

Para un n\'umero finito de eventos $A_1, A_2, \dots, A_n$ son independientes si

\[
\mathbb{P}(A_1\cap\cdots A_n)=\mathbb{P}(A_1)\cdots \mathbb{P}(A_n)
\]


\vspace{0.2cm}

y son independientes por pares si cada par $A_i, A_j,  i \neq j$ son independientes. 

\vspace{0.3cm}

Si generalizamos un poco, dado un conjunto  de eventos $\{ A_i: i \in I \}$ estos  son independientes si



\[
\mathbb{P}\Bigl(  \bigcap\limits_{i \in J}A_i \Bigr)  = \prod_{i \in J}\mathbb{P}(A_i)  
\]

para cada subconjunto $J$ de $I$.



\vspace{0.5cm}

Existe cierta confusi\'on entre eventos independientes y eventos mutuamente exclusivos. A menudo se habla de estos como \texttt{no tener ning\'un efecto sobre el otro}, pero eso no es una caracterizaci\'on precisa en ambos casos. Debes tener  en cuenta que aunque los eventos mutuamente exclusivos no pueden ocurrir juntos, los eventos independientes deben ser capaces de ocurrir juntos.

\vspace{0.2cm}

Supongamos que ni $\mathbb{P}(A)$ ni $\mathbb{P}(B)$ es $0$ y que $A$ y $B$ son mutualmente exclusivos. Entonces $\mathbb{P}(A \cap B) = 0  \neq \mathbb{P}(A)\mathbb{P}(B)$. As\'i $A$ y $B$ no son independientes. Esto es equivalente  a la declaraci\'on que si $A$ y $B$ no pueden ser mutualmente exclusivos.

\end{defi}


\vspace{0.2cm}

\begin{ejemplo}
\normalfont Una moneda es lanzada $4$ veces. Consideremos los eventos $A$ donde la primera moneda muestra cara; $B$ la tercera moneda muestra sello; y $C$ hay un n\'umero igual de caras y sellos. ?` Esos son eventos independientes ?.

\vspace{0.8cm}

Supongamos que el espacio muestral consiste de $16$ puntos que muestran los lanzamientos. El espacio muestral, indicando los eventos que ocurren en cada punto, es como sigue

\vspace{0.2cm}

\begin{table}[h]
\centering
\begin{tabular}{ccc}
Puntos & & Eventos \\
\hline
C C C C   & & A       \\
C C C S   & & A       \\
C C S C   & & A,B     \\
S C C C   & &        \\
C S C C   & & A       \\
C C S S   & & A,B ,C  \\
C S C S   & & A,C     \\
S C C S   & & C       \\
S C S C   & & B,C     \\
C S S C   & & A,B,C   \\
S S C C   & & C       \\
S S S C   & & B       \\
S S C S   & &       \\
S C S S   & &B       \\
C S S S   & &A,B     \\
S S S S   & & B  \\
\hline
\end{tabular}
\end{table}

\vspace{0.2cm}

Entonces $\mathbb{P}(A) = 1/2$ y $\mathbb{P}(B) = 1/2$, mientras que $C$ consiste de $6$ puntos con exactamente dos caras y dos sellos. As\'i $P(C) = 6/16 = 3/8$. Ahora $\mathbb{P}(A \cap B) =\frac{4}{16} = \frac{1}{4} = \mathbb{P}(A)\mathbb{P}(B); \mathbb{P}(A \cap C) = \frac{3}{16} = \mathbb{P}(A) \mathbb{P}(C)$ y $\mathbb{P}(B \cap C) = \frac{3}{16} =  \mathbb{P}(B) \mathbb{P}(C)$. Por tanto los eventos $A, B$ y $C$ son independientes por pares.

\vspace{0.2cm}

El evento $A \cap B \cap C$ consiste de dos puntos $CSSC$ y $CCSS$ con probabilidad $2/16 = 1/8$. As\'i $\mathbb{P}(A \cap B \cap C) \neq \mathbb{P}(A) \mathbb{P}(B) \mathbb{P}(B)$ y as\'i $A, B$ y $C$ no son independientes.
\end{ejemplo}


\vspace{0.3cm}

La siguiente definici\'on generaliza la independencia de una colecci\'on arbitraria de eventos, indexados por un conjunto (potencialmente infinito) $\Theta$.

\vspace{0.2cm}

\begin{defi}
\normalfont M\'ultiples eventos $A_{\theta}, \theta \in \Theta$ son independientes por pares si cada par de eventos es independiente.  M\'ultiples eventos $A_{\theta}, \theta \in \Theta$ son independientes si para cada $k > 0$ y para cada k-subconjunto de distintos eventos $A_{\theta_1},\ldots,A_{\theta_k}$, tenemos

\[
\mathbb{P}(A_{\theta_1}\cap\ldots\cap A_{\theta_k})=\mathbb{P}(A_{\theta_1})\cdots \mathbb{P}(A_{\theta_k})
\]
\end{defi}

\vspace{0.3cm}

La independencia puede surgir de dos maneras distintas. Por ejemplo al lanzar una moneda dos veces, asumimos que los lanzamientos son independientes, lo que se \texttt{traduce}, en que las monedas \texttt{no tienen memoria del primer lanzamiento}. En otro caso, derivamos la independencia verificando $\mathbb{P}(A\cap B) = \mathbb{P}(A)\mathbb{P}(B)$. Por ejemplo, en el lanzamiento de un dado, sea $A = \{ 2, 4, 6 \}$ y sea $B = \{1, 2, 3, 4\}$, entonces $A \cap B = \{2, 4\},  \mathbb{P}(A\cap B) = 2/6 = \mathbb{P}(A)\mathbb{P}(B) = (1/2)(2/3)$ y as\'i $A$ y $B$ son independientes. En este casono asumimos que $A$ y $B$ son independientes.

\vspace{0.2cm}

Supongamos que $A$ y $B$ son eventos disjuntos, cada uno con probabilidad positiva. ?` Pueden ser independientes? No. Esto se debe a que $\mathbb{P}(A)\mathbb{P}(B)> 0$ y $\mathbb{P}(A\cap B) = \mathbb{P}(\emptyset) = 0$. Excepto en este caso especial, no hay manera de revisar  la independencia mirando los conjuntos en un diagrama de Venn .


\vspace{0.5cm}

\begin{ejemplo}
\normalfont Lanzamos una moneda $10$ veces. Sea el evento $A = \texttt{al menos una cara}$. Sea $T_j$ el evento  que salga sello  en el  \mbox{j-\'esimo } lanzamiento. Entonces

\begin{align*}
\mathbb{P}(A) =& 1-\mathbb{P}(A^C)\\
     =& 1 - \mathbb{P}(\mbox{todos los sellos})\\
     =& 1 - \mathbb{P}(T_1T_2 \cdots T_{10})\\
     =&  1- \mathbb{P}(T_1)\mathbb{P}(T_2)\cdots \mathbb{P}(T_{10})  \  \mbox {usando independencia}\\
     =& 1 - \Bigl(\frac{1}{2}\Bigr)^{10}\approx .999.
\end{align*}

\end{ejemplo}

\vspace{0.3cm}

\begin{ejemplo}
\normalfont  Sean dos personas que se turnan para encestar una pelota de baloncesto. La persona $1$ tiene \'exito con probabilidad $1/3$, la  persona 2 tiene \'exito con probabilidad $1/4$. ?` Cu\'al es la probabilidad de que la persona $1$ tenga  \'exito antes de la  persona $2$?. Sea $E$  el evento de inter\'es. Sea $A_j$ el evento en  que el primer \'exito es de la  persona $1$ y que se produce en el n\'umero de lanzamiento $j$. Los conjuntos $A_i$ son disjuntos y forman una partici\'on para $E$, as\'i

\[
\mathbb{P}(E) = \sum_{j = 1}^{\infty}\mathbb{P}(A_j)
\]

\vspace{0.2cm}

Ahora $\mathbb{P}(A_1) = 1/3$. $A_2$ ocurre si tenemos la secuencia persona 1 pierde, persona 2 pierde y la persona 1 tiene \'exito. Luego $\mathbb{P}(A_2) = (2/3)(3/4)(1/3) = (1/2)(1/3)$. Siguiendo esa l\'ogica tenemos que $\mathbb{P}(A_j) = (1/2)^{j -1}(1/3)$. As\'i

\vspace{0.2cm}

\[
\mathbb{P}(E) = \sum_{j  =1}^{\infty}\frac{1}{3}\Bigl(\frac{1}{2}\Bigr)^{j -1} = \frac{1}{3}\sum_{j = 1}^{\infty}\Bigl(\frac{1}{2}\Bigr)^{j -1} = \frac{2}{3}.
\]

\end{ejemplo}

\vspace{0.3cm}

\begin{pros}
\normalfont Si $A$ y $B$ son eventos independientes, entonces $\mathbb{P}(A|B) = \mathbb{P}(A)$. Tambi\'en para alg\'un par de eventos $A$ y $B$

\[
\mathbb{P}(A \cap B) = \mathbb{P}(A|B)\mathbb{P}(B) = \mathbb{P}(B|A)\mathbb{P}(A)
\]

\end{pros}
\vspace{0.2cm}

\begin{ejemplo}
\normalfont Se lanza  dos cartas de una baraja, sin reemplazo. Sea $A$ el evento de que el primera carta  es el as de tr\'eboles y sea $B$ el evento de que la segunda corta  es la reina de diamantes. Entonces $\mathbb{P}(A \cap B) = \mathbb{P}(A)\mathbb{P}(B|A) = (1/52) \times (1/51).$

<<foo8, comment = NA, prompt =TRUE, eval= TRUE>>=
(1/51) * (1/52)
@

\end{ejemplo}


\begin{pros}
\normalfont Si $A, B$ son independientes, entonces son independientes los eventos $A^c, B$, los eventos $A, B^c$ y los eventos $A^c, B^c$. 
\end{pros}


\section{Referencias}

\begin{enumerate}
\item Introduction to Probability and Statistics, Lecture 4 Bayes Law KC Border 2016.
\item Probability (chapter 1) All of Statistics A  concise course in Statistical Inference Larry Wassermann Springer 2004. 
\end{enumerate}
\end{document}
