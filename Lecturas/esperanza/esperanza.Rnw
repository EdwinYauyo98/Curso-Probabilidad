\documentclass{article}
\usepackage[sc]{mathpazo}
\usepackage[T1]{fontenc}
\usepackage{bigints}
\usepackage[latin1]{inputenc}
\usepackage{geometry}
\usepackage{amsmath} 
\usepackage{mathtools}
%\usepackage[spanish]{babel}
\usepackage{latexsym}
\geometry{verbose,tmargin=1.5cm,bmargin=2.5cm,lmargin=2.5cm,rmargin=2.5cm}
\setcounter{secnumdepth}{2}
\setcounter{tocdepth}{2}
\usepackage{url}
\usepackage{subfig}
\usepackage{graphicx}
\newcommand{\R}{\mathbb{R}}
\newcommand{\bb}[1]{\textbf{#1}}
\DeclareMathOperator{\rank}{\textbf{rango}}
\DeclareMathOperator{\proy}{\textbf{proy}}
\DeclareMathOperator{\nulll}{\textbf{nul}}
\DeclareMathOperator{\diag}{\textbf{diag}}
\DeclareMathOperator{\col}{\textbf{col}}
\DeclareMathOperator{\fila}{\textbf{fila}}
\DeclareMathOperator{\dimm}{dim}
\DeclareMathOperator{\Traz}{Tr}
%\theoremstyle{definition}
\everymath{\displaystyle}
\newtheorem{ejemplo}{{Ejemplo }}[section]
\newtheorem{teo}{{Teorema}}[section]
\newtheorem{defi}{{Definici\'on}}[section]
\newtheorem{pros}{{Proposici\'on}}[section]
\newtheorem{cor}{{Corolario}}[section]
\usepackage[unicode=true,pdfusetitle,
 bookmarks=true,bookmarksnumbered=true,bookmarksopen=true,bookmarksopenlevel=2,
 breaklinks=false,pdfborder={0 0 1},backref=false,colorlinks=false]
 {hyperref}
\hypersetup{
 pdfstartview={XYZ null null 1}}
\begin{document}

<<setup, include=FALSE, cache=FALSE>>=
library(knitr)
library(ggplot2)
library(grid)
#library(animation)
# set global chunk options
opts_chunk$set(fig.path='figure/minimal-', fig.align='center', fig.show='hold')
options(formatR.arrow=TRUE,width=90)
@

%\title{\underline{\textbf{Notas de mat\'ematica}}}
%\date{}
%\maketitle
\hspace*{0.5\linewidth}
\begin{minipage}{0.6\linewidth}
Curso: C\'alculo de probabilidades CM-1H2\par
Esperanza de variables aleatorias discretas \par
\end{minipage}


\vspace {0.8cm}

\section{Esperanza de una  variable aleatoria}
 
\vspace{0.3cm}

Al igual que las variables en \'algebra, las variables aleatorias tienen valores, pero estos valores s\'olo pueden determinarse a partir de experimentos aleatorios. Para caracterizar todos los resultados posibles de tal experimento usamos funciones de distribuci\'on, que proporcionan una especificaci\'on completa de una variable aleatoria sin la necesidad de definir un espacio de probabilidad subyacente. Especificar funciones de distribuci\'on no plantea dificultades matem\'aticas inherentes. 

\vspace{0.2cm}

Sin embargo, en las aplicaciones con frecuencia no es posible determinar la funci\'on de distribuci\'on completamente. A menudo una representaci\'on m\'as simple es posible, en lugar de especificar toda la funci\'on especificamos un conjunto de \texttt{valores estad\'isticos}. Un valor estad\'istico es el promedio de una funci\'on de una variable aleatoria. Esto puede ser pensado como el valor que se obtiene de promediar los resultados de un gran n\'umero de experimentos aleatorios en el enfoque de frecuencia de la probabilidad  y en  el marco axiom\'atico  dicho promedio corresponde a la operaci\'on  de la \texttt{esperanza} aplicado a una funci\'on de una variable aleatoria.

\vspace{0.2cm}

Como se sabe en  el marco axiom\'atico, una variable aleatoria se caracteriza completamente por su funci\'on de distribuci\'on acumulativa o de forma equivalente, por su funci\'on de masa o de densidad  de  probabilidad. Sin embargo, no siempre es posible y en muchos casos no es necesario, tener esta informaci\'on completa. En algunos casos, un solo n\'umero que captura alguna caracter\'istica promedio es suficiente. Tales n\'umeros o valores estad\'isticos,  incluyen  la \texttt{esperanza}, la \texttt{mediana} y la \texttt{moda} de la variable aleatoria.

\vspace{0.2cm}


La esperanza  de una variable aleatoria se calcula de manera similar  cuando se calcula el valor promedio de un conjunto de n\'umeros. El valor promedio de un conjunto de $n$  n\'umeros se forma multiplicando cada n\'umero por $1/n$ y a\~nadiendo los resultados. La esperanza de una variable aleatoria que toma los valores $\{x_1, x_2, \dots, x_n\}$ es obtenida multiplicando,  cada proporci\'on de veces que ocurre y agreg\'ando al resultado, es decir, se forma $\displaystyle \sum_{i =1}^{n}x_ip_i$.

\vspace{0.2cm}

El \texttt{valor medio} de una variable aleatoria $X$  tambi\'en se le llama  \texttt{esperanza}  de $X$. La mediana de un conjunto de n\'umeros es el  n\'umero (o n\'umeros) que se encuentra en el centro del conjunto una vez que los n\'umeros est\'an dispuestos en orden ascendente o descendente. La \texttt{mediana} de una variable aleatoria $X$ es cualquier n\'umero $m$ que satisface:

\[
\mathbb{P}(X < m) \leq 1/2 \ \ \text{y}\ \ \mathbb{P}(X > m) \leq 1/2,
\]

o equivalentemente,

\[
\mathbb{P}(X < m) = \mathbb{P}(X > m)
\]

\vspace{0.2cm}

La \texttt{moda} es el valor posible m\'as probable. Para una variable aleatoria, es el valor para el cual la funci\'on de masa de probabilidad o la funci\'on de densidad de probabilidad alcanza su m\'aximo. Si $m$ es un modo de una variable aleatoria $X$, entonces $\mathbb{P}(X= m) \geq \mathbb{P}(X =x)$ para todos los valores de $x$ . Al igual que la mediana  y a diferencia del valor medio, una variable aleatoria puede tener m\'ultiples modas.


\begin{ejemplo}
\normalfont Si $X$ es la variable aleatoria que denota el n\'umero de puntos obtenidos en el  lanzamiento de un dado, entonces hay seis modas (ya que cada n\'umero de puntos es igualmente probable y todos los resultados alcanzan el mismo valor m\'aximo de $1/6$), la mediana es cualquier n\'umero en el intervalo $(3, 4)$. S\'olo hay un valor medio, que es igual a $(1 + 2 + 3 + 4 + 5 + 6)/6 = 3.5$.
\end{ejemplo}

\vspace{0.2cm}

La definici\'on de la media y el hecho de que sea un n\'umero \'unico lo convierte, desde un punto de vista matem\'atico, en un valor mucho m\'as atractivo para trabajar que la mediana o la moda. Este valor puede ampliarse f\'acilmente para proporcionar una caracterizaci\'on completa de una variable aleatoria.

\vspace{0.2cm}

De hecho, una variable aleatoria puede ser completamente caracterizada por un conjunto de valores, llamados \texttt{momentos}. Estos momentos se definen en t\'erminos  de la funci\'on de distribuci\'on, pero generalmente no se necesita calcular la funci\'on de distribuci\'on. Puede demostrarse que, si dos variables aleatorias tienen los mismos momentos de todos los \'ordenes, entonces tambi\'en tienen la misma distribuci\'on.


\vspace{0.2cm}

El primer momento, es la media o esperanza de una variable aleatoria $X$ y es dado por la integral de Riemann-Stieltjes:

\[
\mathbb{E}(X) = \bigintss_{-\infty}^{\infty}xdF(x).
\]

\vspace{0.2cm}

Para una variable aleatoria continua (que veremos m\'as adelante), se tiene el siguiente resultado:

\[
\mathbb{E}(X) = \bigintss_{-\infty}^{\infty}xf_X(x) dx.
\]

donde $f_X(x)$ es la funci\'on de densidad de probabilidad de $X$.

\vspace{0.2cm}


Si $X$ es una variable aleatoria discreta cuyos valores $\{x_1, x_2, \dots\}$, tienen las correspondientes probabilidades $\{p_1, p_2, \dots \}$, entonces la integral, es una suma y como hemos indicado anteriormente:

\[
\mathbb{E}(X) = \sum_{i =1}^{\infty}x_ip_i.
\]

\vspace{0.2cm}

Cuando una variable aleatoria discreta toma valores enteros sobre los enteros no negativos, podemos escribir, la esperanza como:

\begin{align*}
\mathbb{E}(X) &= p_1 + 2p_2 + 3p_3 +4p_4+ \cdots \\
 &= (p_1 + p_2 +p_3 + p_4 + \cdots) + (p_2 + p_3 + p_4 + \cdots ) + (p_3 + p_4 + \cdots ) + \cdots,
\end{align*}

Lo que conduce a una interesante f\'ormula:


\[
\mathbb{E}(X) = \sum_{i =1}^{\infty}\mathbb{P}(X \geq i)
\]


\vspace{0.2cm}

\begin{ejemplo}
\normalfont Considera la variable aleatoria $X$ que denota el n\'umero total de puntos obtenidos cuando dos dados  son lanzados simult\'aneamente. Anteriormente, vimos que la funci\'on de masa de probabilidad de $X$ est\'a dada por:

\vspace{0.2cm}

\begin{figure}[h]
\centering
\includegraphics[scale=.55]{e1.png}
\end{figure}

\vspace{0.2cm}

En este caso la esperanza de $X$ es dado por:

\begin{align*}
\mathbb{E}(X) &= \sum_{i =1}^{\infty}x_ip_i = \sum_{i =2}^{12}x_ip_i\\
 &= 2\frac{1}{36} + 3\frac{2}{36} + 4\frac{3}{36}+ 5\frac{4}{36} +  6\frac{5}{36} +  7\frac{6}{36} +  8\frac{5}{36} +  9\frac{4}{36} +  10\frac{3}{36} +  11\frac{2}{36} +  12\frac{1}{36}\\
 &= \frac{252}{36} = 7.0
\end{align*}
\end{ejemplo}

\vspace{0.2cm}

\begin{ejemplo}
\normalfont En un juego, un jugador lanza una moneda sucesivamente hasta que consigue una \texttt{Cara}. Si esto ocurre en el $k$-\'esimo  lanzamiento, el jugador gana $2^k$ soles. Por lo tanto, si el resultado del primer lanzamiento  es cara, el jugador gana $2$ soles. Si el resultado del primer lanzamiento es sello y el  segundo lanzamiento es cara, el gana $4$. Si los resultados de los dos primeros lanzamientos son sellos, pero el tercero sale cara, ganar\'a $8$ soles  y as\'i sucesivamente. La pregunta es, para jugar este juego, ?` cu\'anto debe pagar una persona, que est\'a dispuesta a jugar este juego?.


\vspace{0.2cm}

Para responder a esta pregunta, sea $X$ la cantidad de dinero que el jugador gana. Entonces $X$ es una variable aleatoria con el conjunto de valores posibles $\{2, 4, 8, \dots, 2^k, \dots \}$ y

\[
\mathbb{P}(X = 2^k) = \biggl(\frac{1}{2}\biggr)^k,\ \ k = 1, 2, 3,\dots
\]

Por tanto,

\[
\mathbb{E}(X) = \sum_{k =1}^{\infty}2^k \biggl(\frac{1}{2}\biggr)^k = \sum_{k =1}^{\infty}1 = 1 + 1 +1 +\cdots = \infty.
\]

\vspace{0.2cm}

Este resultado muestra que el juego sigue siendo injusto, incluso si una persona paga la mayor cantidad posible para jugar. En otras palabras, este es un juego en el que uno siempre gana, no importa lo caro que es jugar. Para ver cu\'al es la falla, tenga en cuenta que te\'oricamente este juego no es factible de jugar porque requiere una enorme cantidad de dinero. En la pr\'actica, sin embargo, la probabilidad de que un jugador gane $2^k$ soles para un valor $k \rightarrow 0$. Incluso para valores peque\~nos de $k$, ganar es altamente improbable. Por ejemplo, para ganar $2^{30} = 1. 073. 741. 824$, debes conseguir $29$ sellos en una fila seguida por una cara. La probabilidad de que esto suceda es $1$ en $1.073.741.824$, mucho menos de $1$ en un bill\'on.
\end{ejemplo}

\vspace{0.3cm}

\begin{ejemplo}
\normalfont Sea $X_0$,  la cantidad de lluvia que caer\'a en los Estados Unidos el pr\'oximo dia de Navidad. Para $n>0$, sea $X_n$ la cantidad de lluvia que caer\'a en los Estados Unidos en Navidad, $n$ a\~nos despu\'es. Sea $N$ el menor n\'umero de a\~nos que transcurren antes de una lluvia de Navidad mayor que $X_0$. Supongamos que $P(X_i = X_j) = 0$ si $i \neq j$, los sucesos relativos a la cantidad de lluvia en dias de Navidad de diferentes a\~nos son independientes y los $X_n$ son id\'enticamente distribuidos. Hallar el valor esperado de $N$.

\vspace{0.2cm}

Como el valor $N$ es el primer valor de $n$, para que se cumpla $X_n > X_0$,

\begin{align*}
\mathbb{P}(N > n)& = \mathbb{P}(X_0 > X_1, X_0 > X_2, \dots, X_0 > X_n)\\
 &= \mathbb{P}(\max(X_0, X_1, X_2, \dots, X_n) = X_0) = \frac{1}{n + 1}
\end{align*}

\vspace{0.2cm}

donde la \'ultima igualdad procede de la simetr\'ia, no hay m\'as raz\'on para que el m\'aximo est\'e en $x_0$ que estar en $X_i, 0 \leq i \leq n$. Por tanto,

\vspace{0.2cm}

\[
\mathbb{P}(N =n) = \mathbb{P}(N > n -1)- \mathbb{P}(N >n ) = \frac{1}{n} - \frac{1}{n +1} = \frac{1}{n(n +1)}.
\]


\vspace{0.2cm}

Entonces se sigue que:

\[
\mathbb{E}{(N)} = \sum_{n =1}^{\infty}n\mathbb{P}(N =n) = \sum_{n =1}^{\infty} \frac{n}{n(n + 1)} =  \sum_{n =1}^{\infty} \frac{1}{n + 1} = \infty.
\] 

\vspace{0.3cm}

Se debe notar que $\mathbb{P}(N > n -1) = 1/n$ da la probabilidad que, en los Estados Unidos, tendremos que esperar m\'as, digamos, tres a\~nos para una lluvia de navidad que sea  mayor que $X_0$ y que la probabilidad sea  s\'olo $1/4$ y la probabilidad de que debamos esperar m\'as de nueve a\~nos es s\'olo $1/10$. Incluso con probabilidades tan bajas en promedio, deber\'ia tomar infinitamente muchos a\~nos antes de que tengamos m\'as lluvia en un d\'ia de navidad de la que tendremos el pr\'oximo d\'ia de Navidad.
\end{ejemplo}

\vspace{0.2cm}

\subsection{Propiedades de la esperanza }

\vspace{0.2cm}

Dado una variable aleatoria discreta $X$ con un conjunto posible de valores $A$ y la funci\'on de masa de probabilidad $p_X(x)$, la esperanza de un nueva variable aleatoria $Y$, que es una funci\'on de $X$, es decir  $Y = h(X)$ se puede escribir como:

\vspace{0.2cm}

\begin{equation}
\mathbb{E}(Y) = \mathbb{E}(h(X)) = \sum_{x \in A}h(x)p_X(x).
\end{equation}

\vspace{0.2cm}

En efecto, sea $\Omega$ un espacio muestral. Sea $h :\mathbb{R} \rightarrow \mathbb{R}$, una funci\'on de variable real y $X: \Omega \rightarrow A \subseteq \mathbb{R}$ es una variable aleatoria con el conjunto de posibles de valores $A$. Como se sabe, $h(X)$, la composici\'on de $g$ y $X$ es una funci\'on desde $\Omega$ al conjunto $h(A) = \{h(x): x\in A\}$. As\'i $h(X)$ es una variable aleatoria con un posible conjunto de valores $g(A)$. Ahora por definici\'on de esperanza:

\vspace{0.2cm}

\[
\mathbb{E}(h(X)) = \sum_{z \in h(A)}z\mathbb{P}(h(X) = z)
\]

\vspace{0.2cm}

Sea $h^{-1}(\{z\}) = \{x: h(x) = z\}$. Debemos notar que no aseveramos que $h$ tiene una funci\'on inversa y en este contexto, consideramos el conjunto $\{x: h(x) =z \}$, que es llamado la imag\'en inversa de $z$ y es denotada por $h^{-1}(\{z\})$. Ahora:

\vspace{0.2cm}

\[
\mathbb{P}(h(X) =z) = \mathbb{P}\biggl(X \in h^{-1}(\{z\})\biggr) = \sum_{\{x:x \in h^{-1}(\{z\}) \}}\mathbb{P}(X =x) = \sum_{ \{x: h(x) =z \}}p_X(x).
\]

\vspace{0.2cm}

\begin{align*}
\mathbb{E}(h(X)) &= \sum_{z \in h(A)}z\mathbb{P}(h(X) = z)  = \sum_{z \in h(A)}z\sum_{ \{x: h(x) =z \}}p_X(x)\\
& = \sum_{z \in h(A)}\sum_{ \{x: h(x) =z \}}zp_X(x) = \sum_{z \in h(A)}\sum_{ \{x: h(x) =z \}}h(x)p_X(x)\\
& = \sum_{x \in A}h(x)p_X(x),
\end{align*}

\vspace{0.2cm}

Donde la \'ultima igualdad se sigue del hecho que la suma es sobre $A$ puede ser llevados en dos escenarios: Podemos sumar primero  todos los $x$ con $h(x) =z$ y entonces para todo $z$.

\vspace{0.2cm}

Dos \'utiles propiedades de la esperanza se sigue a continuaci\'on:

\vspace{0.2cm}

\begin{pros}
\normalfont Sea $X$ una variable aleatoria discreta y sea $a, b \in \mathbb{R}$.

\begin{enumerate}
\item Si $\mathbb{P}(X \geq 0) = 1$ y $\mathbb{E}(X) = 0$, entonces $\mathbb{P}(X = 0) =1$.
\item Tenemos que $\mathbb{E}(aX +b) = a\mathbb{E}(X) + b$.
\end{enumerate}
\end{pros}


\vspace{0.2cm}

\begin{ejemplo}
\normalfont La funci\'on de masa de probabilidad  de una variable aleatoria $X$ es dado por:

\[
p_X(x) = \begin{cases}
x/15 & x = 1, 2, 3, 4, 5 \\
0 & \text{en otros casos  }
\end{cases}
\]

\vspace{0.2cm}

Calculemos la esperanza de $X(6 -X)$.


\vspace{0.3cm}

\[
\mathbb{E}[X(6 -X)] = 5\cdot\frac{1}{15} + 8\cdot\frac{2}{15}+  9\cdot\frac{3}{15} +  8\cdot\frac{4}{15} +  5\cdot\frac{1}{15} +  5\cdot\frac{5}{15} = 7.
\]
\end{ejemplo}

\begin{ejemplo}
\normalfont Para la variable aleatoria discreta $R$ que denota el n\'umero de caras obtenidas en tres lanzamientos de una moneda, la funci\'on de masa de probabilidad est\'a dada por:

\vspace{0.2cm}

\begin{figure}[h]
\centering
\includegraphics[scale=.75]{e2.png}
\end{figure}

\vspace{0.2cm}

Consideremos ahora una situaci\'on de juego en la que un jugador pierde $3$ soles si no aparecen caras, pierde $2$ soles si aparece una cara y  no gana nada  si aparecen dos caras,  pero gana $7$ soles si aparecen tres caras. Calculemos el n\'umero medio de soles ganados o p\'erdidos en el juego. Para ello, definimos la variable aleatoria derivada $Y$ como sigue:

\vspace{0.2cm}

\[
Y = h(X) = \begin{cases}
-3, & X = 0,\\
-2, & X =1, \\
0 & X = 2, \\
7, & X =3, \\
0 & \text{en otros caso.}
\end{cases}
\]

\vspace{0.2cm}

Calculemos $\mathbb{E}(Y)$ y descubramos  que el jugador pierde  25 c\'entimos cada vez que juega:

\vspace{0.2cm}

\[
\mathbb{E}(Y) = \sum_{x = 0,1, 2, 3}h(X)p_X(x) = -3 \times 1/8 -2\times 3/8 + 7\times 1/8 = -1/4.
\]
\end{ejemplo}
\end{document}
