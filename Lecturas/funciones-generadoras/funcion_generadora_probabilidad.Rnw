\documentclass{article}
\usepackage[sc]{mathpazo}
\usepackage[T1]{fontenc}
\usepackage{bigints}
\usepackage[latin1]{inputenc}
\usepackage{geometry}
\usepackage{amsmath} 
\usepackage{mathtools}
%\usepackage[spanish]{babel}
\usepackage{latexsym}
\geometry{verbose,tmargin=1.5cm,bmargin=2.5cm,lmargin=2.5cm,rmargin=2.5cm}
\setcounter{secnumdepth}{2}
\setcounter{tocdepth}{2}
\usepackage{url}
\usepackage{graphicx}
\newcommand{\R}{\mathbb{R}}
\newcommand{\bb}[1]{\textbf{#1}}
\DeclareMathOperator{\rank}{\textbf{rango}}
\DeclareMathOperator{\proy}{\textbf{proy}}
\DeclareMathOperator{\nulll}{\textbf{nul}}
\DeclareMathOperator{\diag}{\textbf{diag}}
\DeclareMathOperator{\col}{\textbf{col}}
\DeclareMathOperator{\fila}{\textbf{fila}}
\DeclareMathOperator{\dimm}{dim}
\DeclareMathOperator{\Traz}{Tr}
%\theoremstyle{definition}
\everymath{\displaystyle}
\newtheorem{ejemplo}{{Ejemplo }}[section]
\newtheorem{teo}{{Teorema}}[section]
\newtheorem{defi}{{Definici\'on}}[section]
\newtheorem{pros}{{Proposici\'on}}[section]
\newtheorem{cor}{{Corolario}}[section]
\usepackage[unicode=true,pdfusetitle,
 bookmarks=true,bookmarksnumbered=true,bookmarksopen=true,bookmarksopenlevel=2,
 breaklinks=false,pdfborder={0 0 1},backref=false,colorlinks=false]
 {hyperref}
\hypersetup{
 pdfstartview={XYZ null null 1}}
\begin{document}

<<setup, include=FALSE, cache=FALSE>>=
library(knitr)
library(ggplot2)
library(grid)
#library(animation)
# set global chunk options
opts_chunk$set(fig.path='figure/minimal-', fig.align='center', fig.show='hold')
options(formatR.arrow=TRUE,width=90)
@

%\title{\underline{\textbf{Notas de mat\'ematica}}}
%\date{}
%\maketitle
\hspace*{0.5\linewidth}
\begin{minipage}{0.6\linewidth}
Curso: C\'alculo de probabilidades CM-1H2\par
Funciones generadoras probabilidad \par
\end{minipage}


\vspace {0.5cm}

\section{Resultados}

\vspace{0.2cm}

Sea $-\infty \leq a < b < \infty$ una funci\'on $g: (a ,b) \rightarrow \mathbb{R}$ es \texttt{convexa} si

\vspace{0.2cm}

\[
g([1 -t]u + tv) \geq (1 -t)g(u) + tg(v)
\]

\vspace{0.2cm}

para $t \in [0,1]$ y $u, v \in (a,b)$.


\vspace{0.5cm}

Presentamos el teorema de soporte de hiperplanos que servir\'a para la desigualdad de Jensen, basado en el gr\'afico siguiente

\vspace{0.2cm}

\begin{figure}[h]
\centering
\includegraphics[scale=.55]{j1.png}
\end{figure}

\vspace{0.2cm}

\begin{pros}
\normalfont Sea $g:(a,b) \rightarrow \mathbb{R}$ convexa y sea $w \in (u,v)$. Entonces existe un $\alpha \in \mathbb{R}$ tal que 

\vspace{0.2cm}

\[
g(x) \geq g(w) + \alpha(x -w)\ \ \text{para}\ \ \ x\in (a,b)
\]

\end{pros}

\vspace{0.5cm}

\begin{teo}
\normalfont \texttt{(Desigualdad de Jensen)} Sea $X$ una variable aleatoria que toma valores en el intervalo (posiblemente infinito) $(a ,b)$ tal que $E(X)$ existe y sea $g:(a, b)\rightarrow \mathbb{R}$ una funci\'on convexa tal que $\mathbb{E}\vert g(X) \vert < \infty$. Entonces:

\vspace{0.2cm}

\[
\mathbb{E}(g(X)) \geq g(\mathbb{E}(X)).
\]
\end{teo}
  
\vspace{0.3cm}

Sea $X$ que toma valores en $(a ,b)$ con media $\mu = \mathbb{E}(X)$. Sea $g$ una funci\'on convexa sobre este intervalo satisfaciendo $\mathbb{E}\vert g(X) \vert < \infty$. Por la proposici\'on (2.1) con $w = \mu$ existe un $\alpha \in \mathbb{R}$ tal que $g(x) \geq g(\mu) + \alpha(x -\mu)$. Por tanto $g(X) \geq g(\mu) + \alpha(X -\mu)$. Tomando esperanza en ambos lados, obtenemos $\mathbb{E}(g(X) \geq g(\mu)$.

\vspace{0.2cm}

\begin{ejemplo}
\normalfont La funci\'on $g(x) = -\log x$ es convexa en el intervalo $(0, \infty)$. Por la desigualdad de Jensen aplicada a una variable aleatoria $X$ positiva con media finita,

\vspace{0.2cm}

\[
\mathbb{E}(\log X) \leq \log(\mathbb{E}(X))
\]

\vspace{0.2cm}

Supongamos que $X$ es una variable aleatoria discreta que es igual de probable de tener los valores $x_1, x_2, \dots, x_n$. Entonces:

\vspace{0.2cm}

\[
\mathbb{E}(\log X) =\dfrac{1}{n}\displaystyle\sum_{i = 1}^{n}\log x_i = \log \gamma, \ \ \ \mathbb{E}(X) = \overline{X}
\]

\vspace{0.2cm}

donde: 

\vspace{0.2cm}

\[
\gamma = \Bigl( \prod_{i=1}^{n}x_i\Bigr)^{1/n}, \ \ \ \ \ \  \overline{x} = \dfrac{1}{n}\displaystyle\sum_{i=1}^{n}x_i
\]

\vspace{0.2cm}

son la media geom\'etrica y aritm\'etica de los valores $x_i$ respectivamente, con la propiedad que $\log \gamma \leq \log \overline{X}$ que conduce a que  $\gamma \leq \overline{x}$.

\end{ejemplo}


La desigualdad de Markov se refiere a la siguiente pregunta: si sabes que la esperanza $\mathbb{E}(X)$ de una variable aleatoria no negativa $X$ es finita, ?`qu\'e dice esto acerca de la distribuci\'on de $X$?. Las llamadas \texttt{colas} derecha e izquierda de $X$ son las probabilidades de que $X$ sea grande y positiva (respectivamente, grande y negativa). M\'as espec\'ificamente, la \texttt{cola} (derecha) es la funci\'on $\mathbb{P}(X \geq  t)$ para $t$ grande, con una definici\'on correspondiente para la \texttt{cola} izquierda. Si $X$ es positiva y $\mathbb{E}(X) < \infty$, entonces la \texttt{cola} derecha de $X$ no puede ser demasiada 'grande'.


\vspace{0.2cm}

\begin{teo}
\normalfont (\texttt{Desigualdad de Markov}) Para una variable no negativa $X$

\vspace{0.2cm}

\[
\mathbb{P}(X \geq t) \leq \frac{\mathbb{E}(X)}{t}\ \ \text{para}\ \ t > 0.
\]
\end{teo}

\vspace{0.2cm}


Sea $X$ una variable aleatoria no negativa y sea $t > 0$. Como $X$ es una funci\'on desde $\Omega $ a $[0, \infty)$, tenemos la desigualdad trivial:

\[
X(\omega) = \begin{cases}
t & \text{Si}\ \ X(\omega) \geq t, \\
0 & \text{Si}\ \  X(\omega) < t
\end{cases}
\]

para $\omega \in \Omega$. Podemos escribir, esto como una desigualdad entre la funci\'on $X$ y la funci\'on indicador del evento $A = \{ X \geq t \}$:

\[
X \geq tI_A
\]

\vspace{0.2cm}

Ahora tomando, esperanza y recordando que $\mathbb{E}(I_A) = \mathbb{P}(A)$, tenemos la desigualdad requerida $\mathbb{E}(X) \geq t\mathbb{P}(X \geq t)$.

\vspace{0.3cm}

\begin{ejemplo}
\normalfont Sea $X$ una variable aleatoria. El n\'umero $m$ es llamado una \texttt{mediana} de $X$, si

\[
\mathbb{P}(X < m)\leq \frac{1}{2} \leq \mathbb{P}(X \leq m)
\]
\end{ejemplo}


\section{Funciones generadoras de probabilidad}

\vspace{0.2cm}

Las \texttt{funci\'ones generadoras } son una herramienta \'util para variables aleatorias que toman valores enteros. Los momentos de una variable aleatoria son derivadas de las funciones generadoras. Las funciones generadoras son \'utiles en el entendimiento de la sum de variables aleatorias.

Una manera de registrar una secuencia de n\'umeros reales $u_0, u_1, u_2, \dots$ es escribir una f\'ormula del $n$-\'esimo t\'ermino. Otra manera es escribir, la  funci\'on generadora de la secuencia, definida como  la suma de la serie de potencias:

\vspace{0.2cm}

\[
u_0 + u_1s + u_2s^2 + \cdots
\]

\vspace{0.3cm}

Por ejemplo la secuencia $1 ,2 , 4, 8, \dots$ tiene una funci\'on generadora :

\vspace{0.2cm}

\[
1 + 2s + 4s^2 + \cdots = \displaystyle\sum_{n = 0}^{\infty}(2s)^n = \dfrac{1}{1 -2s}
\]

\vspace{0.3cm}

v\'alida siempre que $\vert s \vert < \frac{1}{2}$. De manera similar, la secuencia $1, 2,3, \dots$ tiene una funci\'on generadora:

\vspace{0.2cm}

\begin{equation}
1 + 2s + 3s^2 + \cdots = \displaystyle\sum_{n = 0}^{\infty}(n +1)s^n = \dfrac{1}{1 -s}^2
\end{equation}

\vspace{0.3cm}

v\'alida siempre que $\vert s \vert < 1$. 

\vspace{0.2cm}

Tales funciones generadoras son \'utiles para tratar con secuencias reales desde que ellas especifican la secuencia de manera \'unica, es decir dada la secuencia $u_0, u_1, u_2, \dots$ podemos encontrar una funci\'on generadora y  viceversa,  si tenemos una funci\'on generadora $U(s)$ con   una serie de Taylor convergente:

\vspace{0.2cm}

\[
U(s) = u_0 + u_1s + u_2s^2 + \cdots
\]

\vspace{0.2cm}

para todo $s$ peque\~no, entonces esta secuencia es \'unica y   $U(s)$ genera la secuencia $u_0, u_1, u_2, \dots$ de manera \'unica.

\vspace{0.2cm}

Podemos definir otro tipos de funciones generadoras, como es el caso de la \texttt{funci\'on generadora exponencial} de la secuencia real $u_0, u_1, u_2, \dots$,  definida como la serie de potencia:


\vspace{0.2cm}

\[
u_0 + u_1s + \frac{1}{2!}u_2s^2 + \frac{1}{3!}u_3s^3 + \cdots
\]

\vspace{0.3cm}

siempre que la series converga.

\vspace{0.2cm}

Cuando operamos con funciones generadoras de secuencias reales, es importante que la serie de potencia subyacente converja para cierto $s\neq 0$ y  mientras este sea el caso, normalmente no vamos a decir para qu\'e valores de $s$ la serie de potencia es absolutamente convergente.  

\vspace{0.2cm}

Por ejemplo, decimos que $(1 - s)^{-2}$ es la funci\'on generadora de la secuencia $1, 2, 3, \dots$, sin referenciar expl\'icitamente  al hecho de que la serie que le corresponde (1) converge absolutamente  cuando $\vert s \vert < 1$.

\vspace{0.5cm}

\begin{ejemplo}

\normalfont La secuencia dada por:

\vspace{0.3cm}

\[
u_n = \begin{dcases}
{{N}\choose {n}} \ \ \text{si}\ \ n = 0,1,2, \dots, N \\
  0 \ \ \ \ \ \ \ \text{en otros casos}.
\end{dcases}
\]

\vspace{0.3cm}

tiene una funci\'on generadora:

\vspace{0.3cm}

\[
U(s) = \displaystyle\sum_{n = 0}^{N}{{N}\choose{n}}s^N = (1 +s)^N.
\]

\end{ejemplo}

\subsection{Variables aleatorias de valor entero}

\vspace{0.2cm}


Muchas variables aleatorias toman valores en el conjunto de los enteros no negativos. Podemos escribir la funci\'on masa  de esta  variable $X$ como un secuencia $p_0, p_1, p_2, \dots$ de n\'umeros, donde

\vspace{0.2cm}

\[
p_k = \mathbb{P}(X = k) \ \ \ \ \text{para todo}\ \ \ \ k = 0,1,2, \dots
\]

\vspace{0.2cm}

satisfaciendo 

\vspace{0.2cm}

\[
p_k \geq 0 \ \ \text{para} \ \  k, \ \ \ \text{y}\ \ \ \displaystyle\sum_{k =0}^{\infty}p_k = 1
\]

\vspace{0.5cm}

\begin{defi}
\normalfont  La \texttt{funci\'on generadora de probabilidad} o \texttt{pgf} de $X$ es la funci\'on $G_{X}(s)$ definida por

\vspace{0.2cm}

\[
G_{X}(s) = p_0 + p_1s + p_2s^2 + \cdots,
\]


\vspace{0.2cm}

para todo valor $s$ para el cual la sumatoria converge absolutamente.

\end{defi}

\vspace{0.2cm}

En otras palabras la funci\'on generadora de probabilidad $G_{X}(s)$ de $X$ es la funci\'on generadora de la secuencia $p_0, p_1, p_2, \dots$,  que cumple con ciertas propiedades:

\begin{itemize}
\item $G_{X}(0) = p_0$ y $G_{X}(1) = 1$
\item $G_{X}(s) = \mathbb{E}(s^X)$, siempre que la esperanza exista. 
\end{itemize}


\vspace{0.2cm}

Desde este \'ultimo resultado $G_X(s)$ existe para todos los valores de $s$ satisfaciendo $\vert s\vert \leq 1$, ya que en este caso, la serie $G_X$ tiene radio de convergencia al menos $1$.

\vspace{0.2cm}


\[
\displaystyle\sum_{k = 0}^{\infty}\vert p_ks^{k}\vert \leq \displaystyle\sum_{k = 0}^{\infty} p_k = 1
\]

\vspace{0.3cm}

\begin{ejemplo}
\normalfont Sea $X$ una variable aleatoria teniendo una distribuci\'on geom\'etrica con par\'ametro $p$. Entonces: $\mathbb{P}(X = k) = pq^{k - 1}$ para $k = 1, 2, 3, \dots$, con $p + q =1$. Entonces $X$ tiene un \texttt{pgf} dado por:

\vspace{0.2cm}

\begin{align*}
G_{X}(s)& = \displaystyle\sum_{k =1}^{\infty}pq^{k -1}s^k \\
&= ps\displaystyle\sum_{k = 0}^{\infty}(qs)^k = \dfrac{ps}{1 -qs} \ \ \ \text{si}\ \ \vert s \vert < q^{-1}.
\end{align*}

\end{ejemplo}

\vspace{0.3cm}

\begin{pros}
\normalfont Si $X$ e $Y$ tienen \texttt{pgf} $G_X$ y $G_Y$  respectivamente, entonces:

\vspace{0.2cm}

\[
G_{X}(s) = G_{Y}(s) \ \ \ \text{para todo } \ \ \ s
\]

\vspace{0.2cm}

si y s\'olo si $\mathbb{P}(X = k) = \mathbb{P}(Y = k)$ para $k = 0,1,2, \dots$. En otras palabras, las variables aleatorias de valores enteros, tienen la misma funci\'on generadora de probabilidad si y s\'olo si tienen la misma funci\'on de masa de probabilidad.

\end{pros}

\vspace{0.2cm}

Para probar esta proposici\'on, s\'olo se necesita probar que $G_X = G_Y$ implica que $\mathbb{P}(X =k) = \mathbb{P}(Y =k)$ para todo $k$. Como $G_{X}$ y $G_{Y}$ tienen radio de convergencia a lo m\'as  $1$ y por tanto tienen una \'unica expansi\'on alrededor del origen:

\vspace{0.2cm}

\[
G_{X}(s) = \displaystyle\sum_{k = 0}^{\infty}s^{k}\mathbb{P}(X = k), \ \ \ G_{Y}(s) = \displaystyle\sum_{k = 0}^{\infty}s^{k}\mathbb{P}(Y = k)
\]

\vspace{0.2cm}

Si $G_{X} = G_{Y}$, esas dos series tienen id\'enticos coeficientes.


\vspace{0.2cm}


Por la proposici\'on anterior,  se puede deducir para el ejemplo anterior que: si $X$ tiene un \texttt{pgf} igual a $ps(1 -qs)^{-1}$, entonces $X$ tiene una distribuci\'on ge\'ometrica, de par\'ametro $p$.


\vspace{0.3cm}

\begin{ejemplo}
\normalfont Sea $p = 1 -q \in [0,1]$ entonces los \texttt{pgf} de algunas distribuciones discretas son:

\begin{enumerate}
\item Si $X$ tiene una distribuci\'on Bernoulli con par\'ametro $p$ entonces $G_{X}(s) =q + ps$.
\item Si $X$ tiene una distribucio\'n Binomial con par\'ametros $n$ y $p$ entonces

\[
G_{X}(s) = \displaystyle\sum_{k = 0}^{n}{{n}\choose{k}}p^kq^{n -k}s^k = (q + ps)^n.
\]

\item Si  $X$ tiene una distribucio\'n de Poisson con par\'ametro $\lambda$ entonces:

\[
G_{X}(s) = \displaystyle\sum_{k = 0}^{n}\frac{1}{k!}\lambda^ke^{-\lambda}s^k = e^{\lambda(s -1)}.
\]

\item Si $X$ tiene una distribuci\'on binomial negativa, con param\'etros $n$ y $p$, entonces

\[
G_{X}(s) = \displaystyle\sum_{k = n}^{\infty}\binom{k -1}{n -1}p^nq^{k -n}s^k = \Biggl(\frac{ps}{1 -qs} \Biggr)^n \ \ \text{si}\ \vert s \vert < q^{-1}.
\]
\end{enumerate}
\end{ejemplo}

\section{Momentos}

\vspace{0.2cm}

Para una variable aleatoria discreta $X$, la esperanza $\mathbb{E}(X)$ indica el \texttt{centro} de la distribuci\'on de $X$. Este es el primer elemento de una secuencia de n\'umeros conteniendo informaci\'on acerca de la distribuci\'on de $X$. Esta secuencia de n\'umeros $\mathbb{E}(X), \mathbb{E}(X^2), \mathbb{E}(X^3), \dots$ es llamada \texttt{momentos} de $X$.

\vspace{0.2cm}

\begin{defi}
\normalfont Sea $k \geq 1$. El \texttt{K-\'esimo momento} de la variable aleatoria discreta $X$ es el valor de $\mathbb{E}(X^k)$.

\end{defi}

\vspace{0.2cm}

Posiblemente los valores m\'as importantes  que surgen a partir de la definici\'on de momentos es la esperanza $\mathbb{E}(X)$ y la varianza de $X$ :

\vspace{0.2cm}

\[
\text{Var}(X) = \mathbb{E}([X - \mathbb{E}(X)]^2)
\]


\vspace{0.2cm}

Una relaci\'on importante  entre la varianza y el momento de  $X$, se puede distinguir a partir de los siguientes resultados:

\vspace{0.2cm}

\begin{align*}
\text{Var}(X) &= \mathbb{E}(X^2 -2X\mathbb{E}(X) + \mathbb{E}(X)^2)\\
&= \mathbb{E}(X^2) -2\mathbb{E}(X)^2 + \mathbb{E}(X)^2 \\
& = \mathbb{E}(X^2) -\mathbb{E}(X)^2.
\end{align*}

\vspace{0.2cm}

Si $X$ es una variable aleatoria con valores enteros no negativos, los momentos de $X$ se pueden encontrar a partir de  de la funci\'on generadora de momentos de  $X$, calculando la derivada de la funci\'on en el punto $s =1$.

\vspace{0.3cm}

\begin{pros}
\normalfont Sea $X$ una variable aleatoria con una funci\'on generadora de probabilidad,  $G_{X}(s)$, la \texttt{r-\'esima} derivada e $G_{X}(s)$ en el punto $s = 1$ es igual $\mathbb{E}(X[X -1] \cdots [X -r +1])$ para $r =1,2, \dots$. Es decir:

\vspace{0.2cm}

\[
G_{X}^{(r)}(1) = \mathbb{E}(X[X -1] \cdots [X -r +1]).
\]

\end{pros}


\vspace{0.3cm}


De la proposici\'on  anterior, calcular los momentos de $X$ se pueden realizar de la siguientes maneras:

\vspace{0.2cm}

\begin{enumerate}
\item $\mathbb{E}(X) = G_{X}^{'}(1)$.
\item $\mathbb{E}(X^2) = \mathbb{E}(X[X -1] +X) = \mathbb{E}(X[X -1]) + \mathbb{E}(X) = G_{X}^{''}(1) + G_{X}^{'}(1)$.
\item $\text{Var}(X) =  G_{X}^{''}(1) + G_{X}^{'}(1) - G_{X}(1)^2$.
\end{enumerate}

\vspace{0.3cm}

\begin{ejemplo}
\normalfont Sea $X$ una variable aleatoria con una distribuci\'on ge\'ometrica con par\'ametro $p \in (0,1)$. El \texttt{pgf}  de $X$ es $G_{X}(s) =ps(1 -qs)^{-1}$ para $\vert s \vert < q^{-1}$, cuando  $p + q = 1$. As\'i:

\begin{align*}
\mathbb{E}(X) = &G_{X}^{'}(1) = \frac{1}{p}\\
\mathbb{E}(X^2) = &G_{X}^{''}(1) + G_{X}^{'}(1) = \frac{q +1}{p^2}\\
\text{Var}(X) = & \frac{q}{p^2}
\end{align*}

\end{ejemplo}

\section{Suma de variables aleatorias independientes}

\vspace{0.2cm}

Muchos de los problemas en Probabilidad, est\'an relacionados con la suma de variables aleatorias. La f\'ormula de convoluci\'on es inconveniente, desde que $n -1$ convoluciones son requeridas para encontrar la funci\'on de masa de probabilidad de la suma de $n$ variables aleatorias independientes y cada operaci\'on puede ser muy complicada.  En este aspecto las funciones generadoras de momentos son un herramienta importante.

\vspace{0.3cm}

\begin{pros}
\normalfont Si $X$ y $Y$ son variables aleatorias independientes, cada una de ellas con valores en $\{0, 1, \dots \}$, entonces su suma tiene una funci\'on generadora de probabilidad:

\vspace{0.2cm}

\[
G_{X + Y}(s) = G_{X}(s)G_{Y}(s)
\]

\end{pros}

\vspace{0.2cm}

En general, la suma $S_n = X_1 + X_2 + \cdots X_n$ de $n$ variables aleatorias independientes, cada una de ellas con valores en $\{0, 1, \dots \}$, tiene una funci\'on generadora de probabilidad:

\vspace{0.2cm}

\[
G_{S_n}(s) = G_{X_1}(s)G_{X_2}(s)\cdots G_{X_n}(s)
\]

\vspace{0.2cm}

\begin{pros}
\normalfont Sea $N$ y $X_1, X_2, \dots$ variables aleatorias identicamente distribuidas eindependientes, con valores en $\{0, 1, 2, \dots \}$ y un \texttt{pgf} com\'un $G_X$, entonces la suma:

\vspace{0.2cm}

\[
S = X_1 + X_2 + \cdots + X_N
\]

\vspace{0.2cm}

tienen una funci\'on generadora de probabilidad:

\vspace{0.2cm}

\[
G_S(s) = G_{N}(G_X(s))
\]

\end{pros}

\vspace{0.3cm}

La f\'ormula anterior proporciona mucha informaci\'on acerca de la suma de variables aleatorias. Por ejemplo, para encontrar la esperanza  de $S$ en la notaci\'on del anterior  preposici\'on, calculamos $G_{S}^{'}$ como sigue:

\vspace{0.2cm}


\[
G_{S}^{'}(s)= G_{N}^{'}(G_{X}(s))G_{X}^{'}(s)
\]

\vspace{0.2cm}

Para $s = 1$ obtenemos:

\vspace{0.2cm}


\[
G_{S}^{'}(1)= G_{N}^{'}(G_{X}(1))G_{X}^{'}(1) = G_{N}^{'}(1)G_{X}^{'}(1)
\]

\vspace{0.3cm}

desde que $G_{X}(1) =1$. Luego como $\mathbb{E}(X) = G_{X}^{'}(1)$, se tiene:

\vspace{0.2cm}

\[
\mathbb{E}(S) = G_{S}^{'}(1) =\mathbb{E}(N)\mathbb{E}(X)
\]

\vspace{0.2cm}

donde $\mathbb{E}(X)$ es la esperanza de una variable aleatoria  $X_i$.

\vspace{0.2cm}

\begin{pros}
\normalfont Supongamos que todos los momentos $\mathbb{E}(X), \mathbb{E}(X^2)\dots$ de la variable aleatoria$X$ existen y que la serie:

\vspace{0.2cm}

\[
\sum_{k = 0}^{\infty}\dfrac{1}{k!}t^k\mathbb{E}(X^k)
\]

\vspace{0.2cm}

es absolutamente convergente para alg\'un $t > 0$. Entonces la secuencia de momentos determina \'unicamente la distribuci\'on de $X$.

\end{pros}

\vspace{0.3cm}

La absoluta convergencia para alg\'un $t$ es una condici\'on suficiente m\'as no necesaria, para los momentos dedeterminar una distribuci\'on subyacente.

\section{Varianza y covarianza}

\vspace{0.2cm}

La varianza de una variable aleatoria $X$ es definida como:

\[
\text{Var}(X) = \mathbb{E}([X- \mu]^2), 
\]

\vspace{0.2cm}

donde $\mu = \mathbb{E}(X)$.

\vspace{0.2cm}

La varianza es una medida de dispersi\'on con respecto a $\mu$, en el sentido de que si $X$ toma valores que difieren considerablemente de $\mu$, entonces el valor de $\vert X- \mu \vert$ es grande y   $\mathbb{E}([X- \mu]^2)$ tiene un valor grande, mientras que si $X$ es cercano al valor de $\mu$, entonces $\vert X- \mu \vert$ tiene un valor que usualmente peque\~no de la misma forma que $\mathbb{E}([X- \mu]^2)$.

\vspace{0.2cm}

En un caso extremo cuando $X$ est\'a \texttt{concentrado} en alg\'un punto, para una variable $Y$ se tiene el siguiente resultado: 

\vspace{0.2cm}

\[
\mathbb{E}(Y^2) = 0 \ \ \text{si y s\'olo si} \ \ \mathbb{P}(Y = 0) =1 
\]

\vspace{0.2cm}

si aplicamos la ecuaci\'on anterior a $Y = X - \mu$, tenemos el siguiente resultado:

\[
\text{Var}(X) = 0 \ \ \text{si s\'olo si} \ \ \mathbb{P}(X = \mu) =1 
\]

\vspace{0.2cm}

de esta forma la varianza cero significa que \texttt{no hay dispersi\'on en lo absoluto}. 

\subsection{Propiedades  de la varianza}

Algunas propiedades de la varianza son:


\begin{itemize}
\item $\text{Var}(X) = \mathbb{E}([X- \mu]^2) = \mathbb{E}(X^2) - \mu^2$ donde $\mathbb{E}(X) = \mu$.
\item $\text{Var}(aX + b) = a^2\text{Var}(X)$.
\end{itemize}

\vspace{0.2cm}

La varianza como medida de dispersi\'on tiene una propiedad no deseable: no es lineal en el sentido de que la varianza de $aX$ es $a^2$ veces la varianza de $X$, por esta raz\'on es preferible trabajar con la \texttt{desviaci\'on est\'andar} de $X$, definido como $\sqrt{\text{Var}(X)}$.

\vspace{0.2cm}

En t\'erminos de $\text{Var}(X)$ y $\text{Var}(Y)$, la expresi\'on de $\text{Var}(X + Y) $ es de la forma:

\[
\text{Var}(X + Y) = \text{Var}(X) + 2\mathbb{E}([X -\mathbb{E}(X)][Y -\mathbb{E}(Y)]) +\text{Var}(Y)
\]

Es conveniente tener una palabra especial para el t\'ermino medio en la \'ultima expresi\'on y para ello definimos la \texttt{covarianza} de  $X$ e $Y$.

\vspace{0.2cm}

\begin{defi}
\normalfont La \texttt{covarianza} de las variables aleatorias $X$ y $Y$ es la cantidad denotada por $cov(X,Y)$ y es dada por:

\vspace{0.2cm}

\[
cov(X,Y) = \mathbb{E}([X -\mathbb{E}(X)][Y -\mathbb{E}(Y)]) = \mathbb{E}(XY) -\mathbb{E}(X)\mathbb{E}(Y)
\]

\vspace{0.2cm}

siempre que las  esperanzas existan.
\end{defi}

\vspace{0.2cm}

Podemos agregar algunas propiedades m\'as, de la varianza y covarianza:

\vspace{0.2cm}

\begin{itemize}
\item $\text{Var}(X +Y) = \text{Var}(X) + 2cov(X,Y) + \text{Var}(Y)$
\item Si $X$ y $Y$ son independientes : $cov(X,Y) = 0$ y $\text{Var}(X + Y) = \text{Var}(X) + \text{Var}(Y)$.
\end{itemize}


\vspace{0.2cm}

La covarianza se utiliza a menudo como una medida de la dependencia de $X$ e $Y$  y la raz\'on de esto es que $cov(X, Y$) es un s\'olo un  n\'umero (en lugar de un objeto complicado como la funci\'on densidad conjunta) que contiene informaci\'on \'util sobre el comportamiento conjunto  de $X$ e $Y$. 

\vspace{0.2cm}

Por ejemplo, si $cov(X, Y)> 0$, entonces $X-\mathbb{E}(X)$ y  $Y-\mathbb{E}(Y)$ pueden tener una buena chance (en alg\'un sentido) de tener el mismo signo.

\vspace{0.2cm}

La principal desventaja de la covarianza como medida de dependencia de $X$ e $Y$ es que no es invariante en la escala: Si $X$ y $Y$ esta\'an medidas en pulgadas y $U$ y $V$ tienen las misma medida en cent\'imetros (tal que $U = \alpha X$ y $V = \alpha Y$, donde $\alpha \sim 2.54$), entonces $cov(U,V) \sim 6cov(X,Y)$ a pesar del hecho de que el par $(X,Y)$ y $(U,V)$ miden la misma cantidad.

\vspace{0.2cm}

Para tratar con esta \texttt{reescala} en  la covarianza  se define lo siguiente:

\vspace{0.2cm}

\begin{defi}
\normalfont \texttt{El coeficiente de correlaci\'on} de las variables aleatorias $X$ y $Y$ es la cantidad $\rho(X,Y)$ definida por:

\vspace{0.2cm}

\[
\rho(X,Y) = \dfrac{cov(X ,Y)}{\sqrt{\text{Var}(X)\text{Var}(Y)}}
\]

\vspace{0.2cm}

siempre que la \'ultima cantidad exista y $\text{Var}(X)\text{Var}(Y) \neq 0$.

\end{defi}

\vspace{0.2cm}

Una propiedad del coeficiente de  correlaci\'on que se pueden deducir de la definici\'on es:  $\rho(aX +b, cY + d) = \rho(X,Y)$, para $a,b, c, d \in \mathbb{R}$ tal que $ac \neq 0$, es decir el coeficiente de correlaci\'on es invariante a escala.  

\vspace{0.2cm}

La correlaci\'on tiene una propiedad importante como medida de dependencia:

\begin{teo}
\normalfont Si $X$ y $Y$ son variables aleatorias, entonces $-1 \leq \rho(X,Y) \leq 1$, siempre que la correlaci\'on exista. 
\end{teo}

\vspace{0.2cm}

La prueba de este teorema, es una aplicaci\'on directa de la siguiente desigualdad:

\vspace{0.2cm}

\begin{teo}
\normalfont (Teorema de Cauchy-Schwarz) Si $U$ y $V$ son variables aleatorias, entonces 

\[
[\mathbb{E}(UV)]^2 \leq \mathbb{E}(U^2)\mathbb{E}(V^2)
\]

\vspace{0.2cm}

siempre que las esperanzas existan.

\end{teo}

\vspace{0.2cm}

Para probar el \texttt{Teorema 5.1}, se coloca  $U = X - \mathbb{E}(X)$ y $V = Y - \mathbb{E}(Y)$, en la desigualdad de Cauchy-Schwarz, para hallar que:

\[
cov(X,Y)^2 \leq \text{Var}(X)\text{Var}(Y).
\]

Lo que concluye con el resultado.

\vspace{0.3cm}

Sea $a = \text{Var}(X)$, $b = 2cov(X,Y)$, $c = \text{Var}(Y)$  y supongamos que $\rho(X,Y) \pm 1$. Entonces tenemos que  $\text{Var}(X)\text{Var}(Y) \neq 0$ y 

\vspace{0.2cm}

\[
b^2 - 4ac = 4\text{Var}(X)\text{Var}(Y)[\rho(X,Y)^2 - 1] = 0
\]

\vspace{0.2cm}

y as\'i la ecuaci\'on cuadr\'atica: $as^2 + bs + c = 0$ tiene dos ra\'ices iguales en $s = \alpha$. Por tanto, $W = \alpha[X -\mathbb{E}(X)] + [Y -\mathbb{E}(Y)]$ satisface

\vspace{0.2cm}

\[
\mathbb{E}(W^2) = a\alpha^2 + b\alpha + c =0
\]

\vspace{0.2cm}

dando que  $\mathbb{P}(W =0) = 1 $ y probando que $Y = -\alpha X + \beta$, donde $\beta = \alpha \mathbb{E}(X) + \mathbb{E}(Y)$.

\vspace{0.3cm}

Un tratamiento m\'as exahustivo distingue los valores de $\pm 1$:

\begin{itemize}
\item $\rho(X,Y) = 1$ si y s\'olo si $P(Y = \alpha X + \beta$) = 1 para alg\'un real $\alpha$ y $\beta$ con $\alpha > 0$.
\item $\rho(X,Y) = -1$ si y s\'olo si $P(Y = \alpha X + \beta$) = 1 para alg\'un real $\alpha$ y $\beta$ con $\alpha < 0$.
\end{itemize}

\vspace{0.3cm}

Se usa $\rho(X,Y)$ como una medida de las dependencias de $X$ y $Y$. Si $X$ y $Y$ tienen varianza distintas de cero, entonces $\rho(X,Y)$ toma alg\'un valor en el intervalo $[-1, 1]$ y este valor debe ser interpretado de la forma en que los valores $-1, 0, 1$ surgen:

\begin{itemize}
\item Si $X$ y $Y$ son independientes entonces $\rho(X,Y) = 0$.
\item $Y $ es una funci\'on creciente de $X$ si y s\'olo si $\rho(X,Y) = 1$.
\item $Y $ es una funci\'on decreciente de $X$ si y s\'olo si $\rho(X,Y) = -1$.
\end{itemize}

\vspace{0.2cm}

Si $\rho(X,Y) = 0$  se dice que $X$ y $Y$ son \texttt{no correlacionados}.

\section{Funciones generadoras de momentos}

\vspace{0.2cm}

Si $X$ es una variable aleatoria tomando los valores en $\{ 0,1,2, \dots\}$, una  funci\'on generadora de probabilidad es definida como:

\vspace{0.2cm}

\begin{equation}
G_X(s) = \mathbb{E}(s^X) = \sum_{k = 0}^{\infty}s^{k}\mathbb{P}(X =k)
\end{equation}

\vspace{0.2cm}

Las funciones generadoras de probabilidad son muy \'utiles, pero s\'olo cuando las variables aleatorias toman valores enteros no negativos. Para variables aleatorias mucho m\'as generales,  es necesario hacer una modificaci\'on de la ecuaci\'on anterior. 

\vspace{0.2cm}

\begin{defi}
\normalfont {La funci\'on generadora de momentos (MGF)} de la variable aleatoria de $X$ es la funci\'on $M_X$ definida como:

\vspace{0.2cm}

\[
M_{X}(t) = \mathbb{E}(e^{tX})
\]

\vspace{0.2cm}

para todo $t \in \mathbb{R}$ para el cual la esperanza existe.

\end{defi}

\vspace{0.3cm}

Esta es una modificaci\'on de (2), en el sentido que, si $X$ toma valores en $\{0,1,2, \dots \}$, entonces

\vspace{0.2cm}

\[
M_{X}(t) = \mathbb{E}(e^{tX}) = G_{X}(e^t)
\]

\vspace{0.2cm}

con la sustituci\'on de $s = e^t$. En general:

\vspace{0.3cm}

\[
M_{X}(t) = \mathbb{E}(e^{tX}) =  \sum_{x}e^{tx}\mathbb{P}(X =x)  \ \ \text{si}\ \ X \ \ \text{es discreta}
\]

\vspace{0.2cm}

Con esta suma o integral converga absolutamente en algunos casos. La existencia de $M_X(t)$ puede plantear un problema para valores $t$ distintos de cero.

\vspace{0.2cm}

\begin{ejemplo}
\normalfont Si $X$ tiene una distribuci\'on de Poisson con par\'ametro $\lambda$ y funci\'on de masa de probabilidad: 

\vspace{0.2cm}

\[
p_{X}(x) = \dfrac{\lambda^x e^{-\lambda}}{x!} \ \ , x =0, 1,2, \dots
\]

\vspace{0.2cm}

\[
M_X(t) = \displaystyle\sum_{x = 0}^{\infty}e^{tx}\dfrac{\lambda^{x}e^{-\lambda}}{x!}
\]

Si realizamos el cambio de variable $a = e^t\lambda$, entonces obtenemos $M_X(t) = e^{\lambda(e^t -1)}$.

\end{ejemplo}

\vspace{0.5cm}

La  raz\'on del nombre de funci\'on generadora de momentos es la siguiente la expansi\'on que muestra que $M_{X}(t)$ es la funci\'on generadora exponencial de los momentos de $X$:


\vspace{0.2cm}

\begin{align*}
M_{X}(t) = \mathbb{E}(e^{tX}) = \mathbb{E}\Bigl(1 +tX + \dfrac{1}{2!}(tX)^2 + \cdots \Bigr) \\
 = 1 + t\mathbb{E}(X) + \dfrac{1}{2!}t^2\mathbb{E}(X^2) + \cdots
\end{align*}


\vspace{0.2cm}

\begin{teo}
\normalfont Si $M_{X}(t)$ existe en una vecindad de $0$, entonces para $k = 1, 2 \dots$

\vspace{0.2cm}

\[
\mathbb{E}(X^k) = M_{X}^{(k)}(0)
\]

\vspace{0.2cm}

la \texttt{k-\'esima} derivada de $M_X(t)$ evaluada en $t = 0$.

\end{teo}

\vspace{0.6cm}

Como se ha se\~nalado antes, mucho de la teoria se refiere a la suma de variables aleatorias, que puede ser dif\'icil en la pr\'actica calcular de la distribuci\'on de una suma a partir del conocimiento de las distribuciones de los sumandos y es aqu\'i donde las funciones generadoras de momentos  son muy \'utiles.

\vspace{0.3cm}

Consideremos primero  la funci\'on lineal $aX + b$ de la variable aleatoria $X$. Si $a ,b \in \mathbb{R}$


\vspace{0.2cm}

\begin{align*}
M_{aX +b}(t) = \mathbb{E}(e^{t(aX + b)}) = \mathbb{E}(e^{atX}e^{tb}) \\
= e^{tb}\mathbb{E}(e^{(at)X})
\end{align*}

\vspace{0.2cm}


lo que produce que $M_{aX + b}(t) = e^{tb}M_{X}(at)$.


\vspace{0.3cm}

\begin{teo}
\normalfont  Si $X$ e $Y$ son variables aleatorias independientes, entonces $X + Y$ tiene una funci\'on generadora de momentos:

\vspace{0.2cm}

\[
M_{X +Y}(t) = M_{X}(t)M_{Y}(t)
\]
\end{teo}

\vspace{0.3cm}

Tenemos por independencia y propiedad de la esperanza para la variable aleatoria $X$ y $Y$:

\vspace{0.2cm}

\begin{align*}
M_{X +Y}(t) = \mathbb{E}(e^{t(x + y)}) = \mathbb{E}(e^{tX}e^{tY}) \\
= \mathbb{E}(e^{tX})\mathbb{E}(e^{tY})
\end{align*}

\vspace{0.2cm}

En general la suma $S = X_1 + \cdots + X_n$ de $n$ variables aleatorias independientes tiene una funci\'on generadora de momentos de la forma:

\vspace{0.2cm}

\[
M_{S}(t) = M_{X_1}(t)\cdots M_{X_n}(t)
\]



 \end{document}
