\documentclass{article}
\usepackage[sc]{mathpazo}
\usepackage[T1]{fontenc}
\usepackage{bigints}
\usepackage[latin1]{inputenc}
\usepackage{geometry}
\usepackage{amsmath} 
\usepackage{mathtools}
%\usepackage[spanish]{babel}
\usepackage{latexsym}
\geometry{verbose,tmargin=1.5cm,bmargin=2.5cm,lmargin=2.5cm,rmargin=2.5cm}
\setcounter{secnumdepth}{2}
\setcounter{tocdepth}{2}
\usepackage{url}
\usepackage{graphicx}
\newcommand{\R}{\mathbb{R}}
\newcommand{\bb}[1]{\textbf{#1}}
\DeclareMathOperator{\rank}{\textbf{rango}}
\DeclareMathOperator{\proy}{\textbf{proy}}
\DeclareMathOperator{\nulll}{\textbf{nul}}
\DeclareMathOperator{\diag}{\textbf{diag}}
\DeclareMathOperator{\col}{\textbf{col}}
\DeclareMathOperator{\fila}{\textbf{fila}}
\DeclareMathOperator{\dimm}{dim}
\DeclareMathOperator{\Traz}{Tr}
%\theoremstyle{definition}
\everymath{\displaystyle}
\newtheorem{ejemplo}{{Ejemplo }}[section]
\newtheorem{defi}{{Definici\'on}}[section]
\newtheorem{pros}{{Proposici\'on}}[section]
\newtheorem{cor}{{Corolario}}[section]
\usepackage[unicode=true,pdfusetitle,
 bookmarks=true,bookmarksnumbered=true,bookmarksopen=true,bookmarksopenlevel=2,
 breaklinks=false,pdfborder={0 0 1},backref=false,colorlinks=false]
 {hyperref}
\hypersetup{
 pdfstartview={XYZ null null 1}}
\begin{document}

<<setup, include=FALSE, cache=FALSE>>=
library(knitr)
library(ggplot2)
library(grid)
#library(animation)
# set global chunk options
opts_chunk$set(fig.path='figure/minimal-', fig.align='center', fig.show='hold')
options(formatR.arrow=TRUE,width=90)
@

%\title{\underline{\textbf{Notas de mat\'ematica}}}
%\date{}
%\maketitle
\hspace*{0.5\linewidth}
\begin{minipage}{0.6\linewidth}
Curso: C\'alculo de probabilidad CM-1H2\par
Ley de la probabilidad, teorema de Bayes\par
\end{minipage}


\vspace {0.5cm}

\section {El teorema de Bayes}

\vspace{0.3cm}

\subsection {Ley de la probabilidad total}

\vspace{0.3cm}

Sea $A$ un evento, entonces se sabe que la intersecci\'on de $A$ y el evento universal $\Omega$ es $A$. Se sabe adem\'as que $B$ y su complemento $B^c$ constituye una partici\'on. As\'i

\vspace{0.2cm}

\[
A = A \cap \Omega \ \  \text{y}\ \  B \cup B^c = \Omega
\]

\vspace{0.2cm}

Sustituyendo el segundo resultado en el primero en la ecuaci\'on anterior y aplicando la Ley de Morgan, tenemos

\vspace{0.2cm}

\begin{equation}
A = A \cap ( B \cup B^c) = (A \cap B) \cup (A \cap  B^c).
\end{equation}


\vspace{0.2cm}

Los eventos $(A \cap B)$ y  $ (A \cap B^c)$ son mutualmente exclusivos y de acuerdo al siguiente gr\'afico, se muestra que $B$ y $B^c$ no pueden tener salidas en com\'un, la intersecci\'on de $A$ y $B$ no puede tener salidas en com\'un con la intersecci\'on de $A$ y $B^c$


\begin{figure}[h]
\centering
\includegraphics[width=7cm]{h1}
\end{figure}

\vspace{0.2cm}

Usando el hecho que  $(A \cap B)$ y  $ (A \cap B^c)$ son mutualmente exclusivos y por propiedad de la funci\'on probabilidad, se tiene

\vspace{0.2cm}

\[
\mathbb{P}(A) = \mathbb{P}(A \cap B) + \mathbb{P}(A \cap B^c).
\]


Esto significa que para evaluar la probabilidad de un evento $A$, es suficiente encontrar las probabilidades de la intersecci\'on de $A$ y $B$ y $A$ y $B^c$ y sumarlos. 

\vspace{0.3cm}

En general sean $n$ eventos $B_i, i = 1, 2, \dots, n$, una partici\'on del espacio muestral $\Omega$. Entonces para alg\'un evento $A$, podemos escribir

\vspace{0.2cm}

\[
\mathbb{P}(A) = \sum_{i  =1}^n \mathbb{P}(A \cap B_i), \ \  n \geq 1
\]

Esta es la \underline{ley  de la probabilidad total}. En efecto, los conjuntos $A \cap B_i, i = 1,2, \dots, n$ son mutualmente exclusivos (desde que los $B_i$ lo son) y el hecho que $B_i, i = 1,2, \dots$ es una partici\'on de $\Omega$ implica que

\[
A = \bigcup_{i =1}^{n}A \cap B_i,\ n \geq 1,
\]

y por propiedad de la funci\'on probabilidad (tercer axioma), se tiene

\vspace{0.2cm}

\[
\mathbb{P}(A) = \mathbb{P}\Biggl\{ \bigcup_{i = 1}^n A \cap B_i \Biggr\} = \sum_{i =1}^{n}\mathbb{P}(A \cap B_i).
\]

\vspace{0.2cm}


\begin{ejemplo}
\normalfont Consideramos la siguiente figura que muestra una partici\'on de un espacio muestral conteniendo $24$ equiprobables salidas en seis eventos $B_1$ hasta $B_6$.

\vspace{0.2cm}

\begin{figure}[h]
\centering
\includegraphics[width=6cm]{h2}
\end{figure}

Se sigue entonces que la probabilidad del evento $A$ es igual a $1/4$ ya que contiene seis de los puntos de muestreo. Debido a que los eventos $B_i$ constituyen una partici\'on, cada punto de $A$ est\'a en uno y s\'olo uno de los eventos $B_i$ y la probabilidad del evento $A$ se pueden encontrar sumando las probabilidades de los eventos $A \cap B_i$ para $i = 1,2, \dots,6$ .

\vspace{0.2cm}

Para este ejemplo particular se puede ver que estas seis probabilidades est\'an dadas por $0, 1/24, 1/12, 0, 1/24$  y $1/12$ que cuando se suman juntas da 1/4.

\end{ejemplo}


\vspace{0.2cm}

La Ley de la probabilidad total, es frecuentemente presentado en otro contexto, uno que involucra la probabilidad condicional

\vspace{0.2cm}

\[
\mathbb{P}(A) = \sum_{i = 1}^{n}\mathbb{P}(A \cap B_i) = \sum_{i = 1}^{n}\mathbb{P}(A|B_i)\mathbb{P}(B_i)
\]

lo que significa que podemos encontrar $\mathbb{P}(A)$ al encontrar primero la probabilidad de $A$ dado $B _i$  para todo $i$  y luego calcular su promedio ponderado.


\vspace{0.2cm}

\begin{ejemplo}
\normalfont Ma\~nana habr\'a lluvia o nieve, pero no ambos, la probabilidad de lluvia es $2/5$
Y la probabilidad de nieve es $3/5$. Si llueve, la probabilidad de que llegue tarde a mi conferencia es $1/5$  mientras que la probabilidad correspondiente en el caso de nieve es $3/5$. ?`Cu\'al es la probabilidad de que llegue tarde?

\vspace{0.2cm}

Sea $A$ el evento en el que se  llega tarde y $B$ sea el evento que llueva. El par $B$ y $B^c$ es una partici\'on del espacio  muestral (ya que exactamente uno de ellos debe ocurrir). Por  la Ley de la probabilidad total

\begin{align*}
\mathbb{P}(A) &= \mathbb{P}(A | B)\mathbb{P}(B) + \mathbb{P}(A|B^c)\mathbb{P}(B^c)\\
& = \frac{1}{5}\times \frac{2}{5} + \frac{3}{5}\times\frac{3}{5} = \frac{11}{25}.
\end{align*}

\end{ejemplo}

\vspace{0.2cm}

\begin{ejemplo}
\normalfont Supongamos que tres cajas contienen  bolas blancas y negras. La primera caja contiene $12$ bolas blancas y tres negras, la segunda contiene cuatro bolas blancas y $16$ negras y la tercera contiene seis bolas blancas y cuatro negras. Se selecciona una caja y se elige una sola bola. La elecci\'on de la caja se hace de acuerdo al lanzamiento de un dado. Si el n\'umero de puntos en el dado es $1$, se selecciona la primera caja, si el n\'umero de puntos es $2$ \'o $3$ se elige la segunda caja, de lo contrario (el n\'umero de puntos es igual a $4$, $5$ \'o $6$) se elige la tercera caja. Supongamos que queremos encontrar $\mathbb{P}(A)$ donde $A$ es el evento en la que una bola blanca es extraida.

\vspace{0.2cm}

En este caso basaremos la partici\'on en las tres cajas. Especificamente, sea $B_i, i = 1, 2, 3$ el evento en la que la caja $i$ es escogida. Entonces $\mathbb{P}(B_1) = 1/6, \mathbb{P}(B_2) = 2/6$ y $\mathbb{P}(B_3) = 3/6$. Aplicando la Ley de la probabilidad total, tenemos

\vspace{0.2cm}

\[
\mathbb{P}(A) = \mathbb{P}(A|B_1)\mathbb{P}(B_1) + \mathbb{P}(A|B_2)\mathbb{P}(B_2) + \mathbb{P}(A|B_3)\mathbb{P}(B_3),
\]

\vspace{0.2cm}

que es f\'acil calcular, usando

\[
\mathbb{P}(A|B_1)= 12/15, \ \ \mathbb{P}(A|B_2)= 4/20,\ \ \mathbb{P}(A|B_3)= 6/10.
\]

\vspace{0.2cm}

as\'i

\vspace{0.2cm}

\[
\mathbb{P}(A) = 12/15 \times 1/6 + 4/20 \times 2/6 + 6/10 \times 3/6 = 1/2.
\]
\end{ejemplo}

\vspace{0.3cm}


\begin{ejemplo}
\normalfont Un mensaje de correo electr\'onico puede viajar a trav\'es de una de las tres rutas de un  servidor. La probabilidad de transmisi\'on de error en cada uno de los servidores y la proporci\'on de mensajes que viajan en  cada ruta se muestran en la siguiente tabla. Suponga que los servidores son independientes.

\begin{table}[h]
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
           & \% mensajes & \% errores \\ \hline
Servidor 1 & 40          & 1          \\ \hline
Servidor 2 & 25          & 2          \\ \hline
Servidor 3 & 35          & 1.5        \\ \hline
\end{tabular}
\end{center}
\end{table}

Determina el porcentaje de mensajes que contienen error

\vspace{0.2cm}

<<foo8K, comment = NA, prompt =TRUE, eval= TRUE>>=
# De la probabilidad total 
0.4 * 0.01 + 0.25*0.02 + 0.35*0.15
@

\end{ejemplo}

\subsection{Teorema de Bayes} 

\vspace{0.3cm}

Con frecuencia ocurre que se nos dice que ha ocurrido un cierto evento $A$ y nos gustar\'ia saber cu\'ales de los eventos mutuamente excluyentes y colectivamente exhaustivos $B_j$ han ocurrido, al menos probabil\'isticamente. En otras palabras, nos gustar\'ia conocer  $\mathbb{P}(B_j|A)$ para cualquier $j$. Consideremos algunos ejemplos.

\vspace{0.2cm}

En un escenario, se nos puede decir que entre una cierta poblaci\'on hay quienes tienen una enfermedad espec\'ifica y aquellos que no tienen  esa enfermedad. Esto  proporciona una partici\'on del espacio muestral (la poblaci\'on) en dos conjuntos disjuntos. Se puede realizar un cierto test, no totalmente fiable en pacientes,  con el objeto de detectar la presencia de esta enfermedad.

\vspace{0.2cm}

Si se conoce la proporci\'on de enfermos respecto de  los pacientes libres de enfermedad y la fiabilidad del procedimiento de prueba, entonces teniendo en cuenta que un paciente es declarado libre de la enfermedad  por este test, queremos  saber la probabilidad de que el paciente tiene en realidad la enfermedad (la probabilidad de que el paciente caiga en el primero (o segundo) de los dos conjuntos disjuntos).

\vspace{0.2cm}

El mismo escenario puede obtenerse sustituyendo chips de circuitos integrados por la poblaci\'on y particion\'andolos en chips defectuosos y no defectuosos, junto con un probador que a veces puede declarar un chip defectuoso bueno y viceversa. Dado que un chip se declara defectuoso por el probabdor, queremos conocer la probabilidad de que sea defectuoso. La transmisi\'on de datos a trav\'es de un canal de comunicaci\'on sujeto a ruido es todav\'ia un tercer ejemplo. En este caso, la partici\'on es la informaci\'on que se env\'ia (normalmente $0$ y $1$ s) y el ruido en el canal puede o no alterar los datos. Escenarios como estos se responden mejor, con la Regla  de Bayes.

\vspace{0.2cm}

Obtenemos la regla de Bayes de resultados anteriores sobre la probabilidad condicional y el teorema de la probabilidad total. As\'i tenemos

\[
\mathbb{P}(B_j|A) = \frac{\mathbb{P}(A \cap B_j)}{\mathbb{P}(A)} = \frac{\mathbb{P}(A|B_j)\mathbb{P}(B_j)}{\sum_{i}\mathbb{P}(A|B_i)\mathbb{P}(B_i)}
\]

\vspace{0.3cm}

Aunque parezca que esto complica las cosas, lo que estamos haciendo es dividir el problema en piezas m\'as simples. Esto se hace obvio en el ejemplo siguiente donde elegimos un espacio de muestra dividido en tres eventos.

\vspace{0.3cm}

\begin{ejemplo}
\normalfont Considere un profesor universitario que observa a los estudiantes que entran en su oficina con preguntas. Este profesor determina que el $60\%$ de los estudiantes son estudiantes de BSc, mientras que el $30\%$ son estudiantes de MS, $10\%$ son estudiantes de doctorado. El profesor tambi\'en indica que puede manejar las preguntas del $80\%$  de los estudiantes BSc en menos de cinco minutos, mientras que las preguntas de $50\%$ de los estudiantes de MS y el  $40\%$ de los estudiantes de doctorado en cinco minutos o menos. 

El pr\'oximo estudiante en  ingresar a la oficina del  profesor  s\'olo necesitaba dos minutos del tiempo del profesor. ?`Cu\'al es la probabilidad de que el estudiante sea un estudiante de doctorado?.

\vspace{0.3cm}

Para responder, esta pregunta, sean los eventos $B_i, i =1,2,3$ \texttt{el estudiante es un BCs, MS y doctorado } respectivamente y sea el evento $A$ el evento \texttt{el estudiante requiere cinco minutos o menos}. 

\vspace{0.2cm}

Desde la ley de probabilidad total, tenemos

\begin{align*}
\mathbb{P}(A) &= \mathbb{P}(A|B_1)\mathbb{P}(B_1) +  \mathbb{P}(A|B_2)\mathbb{P}(B_2) +  \mathbb{P}(A|B_3)\mathbb{P}(B_3) \\
& = 0.8 \times 0.6 + 0.5 \times 0.3 + 0.4\times 0.1 = 0.6700
\end{align*}

\vspace{0.2cm}

Este c\'alculo nos da el denominador para la inserci\'on en la regla de Bayes. Esto nos dice que aproximadamente dos tercios de todas las preguntas de los estudiantes se pueden manejar en cinco minutos o menos.

\vspace{0.2cm}

Calculemos  $\mathbb{P}(B_3|A)$ usando la regla de Bayes, tenemos

\vspace{0.2cm}

\[
\mathbb{P}(B_3|A) = \frac{\mathbb{P}(A|B_3)\mathbb{P}(B_3)}{\mathbb{P}(A)} = \frac{0.4 \times 0.1}{0.6700} = 0.0597
\]

\vspace{0.2cm}

O alrededor del $6\%$ y es la respuesta que buscamos.

\vspace{0.3cm}

El punto cr\'itico en la respuesta a preguntas como estas es determinar qu\'e conjunto de eventos constituye la partici\'on del espacio muestral. Las pistas se encuentran generalmente en la pregunta planteada.

\vspace{0.2cm}

Si recordamos que se nos pide calcular $\mathbb{P}(B_j|A)$ y relacionar esto con las palabras en la pregunta, entonces se hace evidente que las palabras \texttt{era un estudiante de doctorado} sugiere una partici\'on basada en el estatus del estudiante y  el evento $A$, la informaci\'on que se nos da, se refiere al tiempo que toma el estudiante.

\vspace{0.2cm}

La clave est\'a en entender que se nos da $\mathbb{P}(A|B_j)$ y se nos pide encontrar  $\mathbb{P}(B_j|A)$. En esta  forma  simple, la ley de Bayes se escribe como


\vspace{0.2cm}

\[
\mathbb{P}(B|A) = \frac{\mathbb{P}(A|B)\mathbb{P}(B)}{\mathbb{P}(A)}.
\]
\end{ejemplo}


\vspace{0.3cm}

\begin{ejemplo}
\normalfont Un experimentador tienes dos urnas A y B a su disposici\'on. La urna A (B) tiene ocho (diez) canicas verdes y doce(ocho) canicas azules, todas del mismo tama\~no y peso. El experimentador selecciona una urna con igual probabilidad y escoge una canica al azar. Se anuncia  que la canica seleccionada es azul. Encontremos la probabilidad de que esta canica provenga de la urna B.

\vspace{0.3cm}

Definamos los eventos

\vspace{0.2cm}

$A_i$: La urna $i $ es seleccionada, $i = 1,2$.

$B$ : La canica escogida desde la urna seleccionada es azul.

\vspace{0.2cm}

Entonces $\mathbb{P}(A_i) = \frac{1}{2}$ para $i = 1, 2$, mientras que $\mathbb{P}(B|A_1) = \frac{12}{20}$ y $\mathbb{P}(B|A_2) = \frac{8}{18}$. 

\vspace{0.2cm}

Ahora apliquemos el teorema de Bayes,

\vspace{0.2cm}

\[
\mathbb{P}(A_2|B) = \mathbb{P}(A_2)P(B|A_2) /\{ \mathbb{P}(A_1)P(B|A_1) + \mathbb{P}(A_2)\mathbb{P}(B|A_2) \} 
\]

\vspace{0.2cm}

Reemplazando:

\vspace{0.2cm}

<<foo8v, comment = NA, prompt =TRUE, eval= TRUE>>=
(1/2)*(8/18)/((1/2)*(12/20) + (1/2)*(8/18))
@

\end{ejemplo}

\section{Referencias}

\begin{enumerate}
\item Introduction to Probability and Statistics, Lecture 4 Bayes Law KC Border 2016.
\item Probability (chapter 1) All of Statistics A  concise course in Statistical Inference Larry Wassermann Springer 2004. 
\end{enumerate}
\end{document}
