\documentclass{article}
\usepackage[sc]{mathpazo}
\usepackage[T1]{fontenc}
\usepackage{bigints}
\usepackage[latin1]{inputenc}
\usepackage{geometry}
\usepackage{amsmath} 
\usepackage{mathtools}
%\usepackage[spanish]{babel}
\usepackage{latexsym}
\geometry{verbose,tmargin=1.5cm,bmargin=2.5cm,lmargin=2.5cm,rmargin=2.5cm}
\setcounter{secnumdepth}{2}
\setcounter{tocdepth}{2}
\usepackage{url}
\usepackage{graphicx}
\newcommand{\R}{\mathbb{R}}
\newcommand{\bb}[1]{\textbf{#1}}
\DeclareMathOperator{\rank}{\textbf{rango}}
\DeclareMathOperator{\proy}{\textbf{proy}}
\DeclareMathOperator{\nulll}{\textbf{nul}}
\DeclareMathOperator{\diag}{\textbf{diag}}
\DeclareMathOperator{\col}{\textbf{col}}
\DeclareMathOperator{\fila}{\textbf{fila}}
\DeclareMathOperator{\dimm}{dim}
\DeclareMathOperator{\Traz}{Tr}
%\theoremstyle{definition}
\everymath{\displaystyle}
\newtheorem{ejemplo}{{Ejemplo }}[section]
\newtheorem{teo}{{Teorema}}[section]
\newtheorem{defi}{{Definici\'on}}[section]
\newtheorem{pros}{{Proposici\'on}}[section]
\newtheorem{cor}{{Corolario}}[section]
\usepackage[unicode=true,pdfusetitle,
 bookmarks=true,bookmarksnumbered=true,bookmarksopen=true,bookmarksopenlevel=2,
 breaklinks=false,pdfborder={0 0 1},backref=false,colorlinks=false]
 {hyperref}
\hypersetup{
 pdfstartview={XYZ null null 1}}
\begin{document}

<<setup, include=FALSE, cache=FALSE>>=
library(knitr)
library(ggplot2)
library(grid)
#library(animation)
# set global chunk options
opts_chunk$set(fig.path='figure/minimal-', fig.align='center', fig.show='hold')
options(formatR.arrow=TRUE,width=90)
@

%\title{\underline{\textbf{Notas de mat\'ematica}}}
%\date{}
%\maketitle
\hspace*{0.5\linewidth}
\begin{minipage}{0.6\linewidth}
Curso: C\'alculo de probabilidad CM-1H2\par
Momentos y transformada de Laplace\par
\end{minipage}



\section{Momentos}

\vspace{0.2cm}

Para una variable aleatoria discreta $X$, la esperanza $\mathbb{E}(X)$ indica el \texttt{centro} de la distribuci\'on de $X$. Este es el primer elemento de una secuencia de n\'umeros conteniendo informaci\'on acerca de la distribuci\'on de $X$. Esta secuencia de n\'umeros $\mathbb{E}(X), \mathbb{E}(X^2), \mathbb{E}(X^3), \dots$ es llamada \texttt{momentos} de $X$.

\vspace{0.2cm}

\begin{defi}
\normalfont Sea $k \geq 1$. El \texttt{K-\'esimo momento} de la variable aleatoria discreta $X$ es el valor de $\mathbb{E}(X^k)$.

\end{defi}

\vspace{0.2cm}

Posiblemente los valores m\'as importantes  que surgen a partir de la definici\'on de momentos es la esperanza $\mathbb{E}(X)$ de $X$ y la varianza de $X$ :

\vspace{0.2cm}

\[
\text{Var}(X) = \mathbb{E}([X - \mathbb{E}(X)]^2)
\]


\vspace{0.2cm}

Una relaci\'on importante  entre la varianza y el momento de  $X$, se puede distinguir a partir de los siguientes resultados:

\vspace{0.2cm}

\begin{align*}
\text{Var}(X) &= \mathbb{E}(X^2 -2X\mathbb{E}(X) + \mathbb{E}(X)^2)\\
&= \mathbb{E}(X^2) -2\mathbb{E}(X)^2 + \mathbb{E}(X)^2 \\
& = \mathbb{E}(X^2) -\mathbb{E}(X)^2.
\end{align*}

\vspace{0.2cm}

Si $X$ es una variable aleatoria con valores enteros no negativos, los momentos de $X$ se pueden encontrar a partir de  de la funci\'on generadora de momentos de  $X$, calculando la derivada de la funci\'on en el punto $s =1$.

\vspace{0.3cm}

\begin{pros}
\normalfont Sea $X$ una variable aleatoria con una funci\'on generadora de probabilidad,  $G_{X}(s)$, la \texttt{r-\'esima} derivada e $G_{X}(s)$ en el punto $s = 1$ es igual $\mathbb{E}(X[X -1] \cdots [X -r +1])$ para $r =1,2, \dots$. Es decir:

\vspace{0.2cm}

\[
G_{X}^{(r)}(1) = \mathbb{E}(X[X -1] \cdots [X -r +1]).
\]

\end{pros}


\vspace{0.3cm}


De la proposici\'on  anterior, calcular los momentos de $X$ se pueden realizar de la siguientes maneras:

\vspace{0.2cm}

\begin{enumerate}
\item $\mathbb{E}(X) = G_{X}^{'}(1)$.
\item $\mathbb{E}(X^2) = \mathbb{E}(X[X -1] +X) = \mathbb{E}(X[X -1]) + \mathbb{E}(X) = G_{X}^{''}(1) + G_{X}^{'}(1)$.
\item $\text{Var}(X) =  G_{X}^{''}(1) + G_{X}^{'}(1) - G_{X}(1)^2$.
\end{enumerate}

\vspace{0.3cm}

\begin{ejemplo}
\normalfont Sea $X$ una variable aleatoria con una distribuci\'on ge\'ometrica con par\'ametro $p \in (0,1)$. El \texttt{pgf}  de $X$ es $G_{X}(s) =ps(1 -qs)^{-1}$ para $\vert s \vert < q^{-1}$, cuando  $p + q = 1$. As\'i:

\begin{align*}
\mathbb{E}(X) = &G_{X}^{'}(1) = \frac{1}{p}\\
\mathbb{E}(X^2) = &G_{X}^{''}(1) + G_{X}^{'}(1) = \frac{q +1}{p^2}\\
\text{Var}(X) = & \frac{q}{p^2}
\end{align*}

\end{ejemplo}

\vspace{0.5cm}

\subsection{Suma de variables aleatorias independientes}

\vspace{0.2cm}

Muchos de los problemas en Probabilidad, est\'an relacionados con la suma de variables aleatorias. La f\'ormula de convoluci\'on es inconveniente, desde que $n -1$ convoluciones son requeridas para encontrar la funci\'on de masa de probabilidad de la suma de $n$ variables aleatorias independientes y cada operaci\'on puede ser muy complicada.  En este aspecto las funciones generadoras de momentos son un herramienta importante.

\vspace{0.3cm}

\begin{pros}
\normalfont Si $X$ y $Y$ son variables aleatorias independientes, cada una de ellas con valores en $\{0, 1, \dots \}$, entonces su suma tiene una funci\'on generadora de probabilidad:

\vspace{0.2cm}

\[
G_{X + Y}(s) = G_{X}(s)G_{Y}(s)
\]

\end{pros}

\vspace{0.2cm}

En general, la suma $S_n = X_1 + X_2 + \cdots X_n$ de $n$ variables aleatorias independientes, cada una de ellas con valores en $\{0, 1, \dots \}$, tiene una funci\'on generadora de probabilidad:

\vspace{0.2cm}

\[
G_{S_n}(s) = G_{X_1}(s)G_{X_2}(s)\cdots G_{X_n}(s)
\]

\vspace{0.2cm}

\begin{pros}
\normalfont Sea $N$ y $X_1, X_2, \dots$ variables aleatorias identicamente distribuidas eindependientes, con valores en $\{0, 1, 2, \dots \}$ y un \texttt{pgf} com\'un $G_X$, entonces la suma

\vspace{0.2cm}

\[
S = X_1 + X_2 + \cdots + X_N
\]

\vspace{0.2cm}

tienen una funci\'on generadora de probabilidad:

\vspace{0.2cm}

\[
G_S(s) = G_{N}(G_X(s))
\]

\end{pros}

\vspace{0.3cm}

La f\'ormula anterior proporciona mucha informaci\'on acerca de la suma de variables aleatorias. Por ejemplo, para encontrar la esperanza  de $S$ en la notaci\'on del anterior  preposici\'on, calculamos $G_{S}^{'}$ como sigue

\vspace{0.2cm}


\[
G_{S}^{'}(s)= G_{N}^{'}(G_{X}(s))G_{X}^{'}(s)
\]

\vspace{0.2cm}

Para $s = 1$ obtenemos:

\vspace{0.2cm}


\[
G_{S}^{'}(1)= G_{N}^{'}(G_{X}(1))G_{X}^{'}(1) = G_{N}^{'}(1)G_{X}^{'}(1)
\]

\vspace{0.3cm}

desde que $G_{X}(1) =1$. Luego como $\mathbb{E}(X) = G_{X}^{'}(1)$, se tiene:

\vspace{0.2cm}

\[
\mathbb{E}(S) = G_{S}^{'}(1) =\mathbb{E}(N)\mathbb{E}(X)
\]

\vspace{0.2cm}

donde $\mathbb{E}(X)$ es la esperanza de una variable aleatoria  $X_i$.


\vspace{0.5cm}

\section{Caso general de momentos}

\vspace{0.2cm}

Para una variable aleatoria $X$ el \texttt{k-\'esimo momento} de $X$ es definido para $k = 0, 1,2, \cdots$ como el  n\'umero $\mathbb{E}(X^k)$ que es la esperanza de la \texttt{k-\'esima potencia } de $X$ siempre que la esperanza  exista.

\vspace{0.2cm}

La secuencia $\mathbb{E}(X), \mathbb{E}(X^2), \dots$ contiene mucha informaci\'on acerca de la distribuci\'on de $X$.

\vspace{0.2cm}


\begin{ejemplo}
\normalfont Si $X$ tienen una distribuci\'on exponencial con param\'etro $\lambda$, entonces:

\vspace{0.2cm}

\begin{align*}
\mathbb{E}(X^k) &= \bigintsss_{0}^{\infty}x^k\lambda e^{-\lambda x}dx \\
&= \frac{k}{\lambda}\mathbb{E}(X^{k -1})
\end{align*}

\vspace{0.2cm}

Si $k \geq 1$, tenemos el siguiente resultado:

\vspace{0.2cm}

\begin{align*}
\mathbb{E}(X^k) = \frac{k}{\lambda}\mathbb{E}(X^{k -1}) &= \frac{k(k -1)}{\lambda^2}\mathbb{E}(X^{k -2}) = \cdots \\
&= \frac{k!}{\lambda^k}\mathbb{E}(X^{0}) = \frac{k!}{\lambda^k}\mathbb{E}(1) = \frac{k!}{\lambda^k}
\end{align*}

\vspace{0.2cm}

En particular, la distribuci\'on exponencial tienen momentos de todos los \'ordenes.

\end{ejemplo}

\vspace{0.3cm}

\begin{ejemplo}
\normalfont Si $X$ tiene una distribuci\'on de Cauchy, entonces

\vspace{0.2cm}

\[
\mathbb{E}(X^k) = \bigintsss_{-\infty}^{\infty}\dfrac{x^k}{\pi(1 + x^2)}dx
\]

\vspace{0.2cm}

para valores de $k$ para el cual, la integral converge absolutamente. Pero se cumple que:

\vspace{0.2cm}

\[
\bigintsss_{-\infty}^{\infty}\Bigl\vert\dfrac{x^k}{\pi(1 + x^2)}\Bigr\vert dx = \infty
\]

\vspace{0.2cm}

si $k \geq 1$. Entonces la distribuci\'on de Cauchy no tiene momentos.

\end{ejemplo}

\vspace{0.2cm}

Podemos adaptar este ejemplo para encontrar una funci\'on  densidad con algunos momentos, pero no con  todos. Considera la funci\'on de densidad:

\[
f_X(x) = \frac{c}{ 1 + \vert x \vert^m}\ \ \text{para}\ x \in \mathbb{R}.
\]

\vspace{0.2cm}

donde $m(\geq 2)$ es un entero y $c$ es escogido de manera que, en efecto es una funci\'on densidad:

\[
c = \Biggl(\bigints_{-\infty}^{\infty} \frac{dx}{ 1 + \vert x \vert^m}\Biggr)^{-1}.
\]

Se puede verificar que esta funci\'on densidad tiene un k-\'esimo momento, para aquellos valores $k$ satisfaciendo $1 \leq k \leq m -2$ solamente.

\vspace{0.2cm}

Dada la funci\'on de distribuci\'on $F_X$ de la variable aleatoria $X$, podemos calcular los momento siempre y cuando esta funci\'on exista, ya sea $X$ discreta o continua. Lo contrario; es decir ?`dada una secuencia $\mathbb{E}(X), \mathbb{E}(X^2), \dots$ de momentos(finitos) de $X$ es posible reconstruir $X$?. La respuesta en general  no se cumple en general, salvo algunas condiciones extras.

\vspace{0.3cm}

\begin{pros}
\normalfont Supongamos que todos los momentos $\mathbb{E}(X), \mathbb{E}(X^2)\dots$ de la variable aleatoria$X$ existen y que la serie

\vspace{0.2cm}

\[
\sum_{k = 0}^{\infty}\dfrac{1}{k!}t^k\mathbb{E}(X^k)
\]

\vspace{0.2cm}

 es absolutamente convergente para alg\'un $t > 0$. Entonces la secuencia de momentos determina \'unicamente la distribuci\'on de $X$.

\end{pros}

\vspace{0.3cm}

La absoluta convergencia para alg\'un $t$ es una condici\'on suficiente m\'as no necesaria, para los momentos para  determinar una distribuci\'on subyacente. Mostremos que  una distribuci\'on que no es determinada por momentos de manera \'unica

\vspace{0.2cm}

\begin{ejemplo}
\normalfont Si $X$ tiene una distribuci\'on normal con media $0$ y varianza $1$, entonces $Y = e^{X}$ tiene una distribuci\'on \texttt{log-normal}  con funci\'on densidad 

\vspace{0.2cm}

\[
f_X(x) = \begin{cases}
\dfrac{1}{x\sqrt{2\pi}}\exp[-\frac{1}{2}(\log x)^2] & \text{si} \ x > 0, \\
0 & \text{en otros casos}
\end{cases}
\]

\vspace{0.2cm}

Suponiendo que $-1 \leq a \leq 1$, definimos:

\vspace{0.2cm}

\[
f_a{x} = [1 + a\sin(2\pi \log x)]f(x)
\]

\vspace{0.2cm}

entonces se cumple:

\vspace{0.2cm}

\begin{itemize}
\item $f_a$ es una funci\'on densidad.
\item $f$ tiene momentos finitos de todos los \'ordenes.
\item Si $f_a$ y $f$ tienen momentos iguales de todos los \'ordenes, cuando

\vspace{0.3cm}

\[
\bigintsss_{-\infty}^{\infty}x^kf(x)dx = \bigintsss_{-\infty}^{\infty}x^kf_a(x)dx \ \ \text{para}\ k =1, 2, \dots
\]
\end{itemize}

\vspace{0.2cm}

As\'i $\{f_a: -1 \leq a \leq 1 \}$ es una colecci\'on de distintas funciones densidad teniendo los mismos momentos. 

\end{ejemplo}

\vspace{0.5cm}

\section{Funciones generadoras de momentos}

\vspace{0.2cm}

Si $X$ es una variable aleatoria tomando los valores en $\{ 0,1,2, \dots\}$, una  funci\'on generadora de probabilidad es definida como:

\vspace{0.2cm}

\begin{equation}
G_X(s) = \mathbb{E}(s^X) = \sum_{k = 0}^{\infty}s^{k}\mathbb{P}(X =k)
\end{equation}

\vspace{0.2cm}

Las funciones generadoras de probabilidad son muy \'utiles, pero s\'olo cuando las variables aleatorias toman valores enteros no negativos. Para variables aleatorias mucho m\'as generales,  es necesario hacer una modificaci\'on de la ecuaci\'on anterior. 

\vspace{0.2cm}

\begin{defi}
\normalfont {La funci\'on generadora de momentos (mgf)} de la variable aleatoria de $X$ es la funci\'on $M_X$ definida como:

\vspace{0.2cm}

\[
M_{X}(t) = \mathbb{E}(e^{tX})
\]

\vspace{0.2cm}

para todo $t \in \mathbb{R}$ para el cual la esperanza existe.

\end{defi}

\vspace{0.3cm}

Esta es una modificaci\'on de (2), en el sentido que, si $X$ toma valores en $\{0,1,2, \dots \}$, entonces

\vspace{0.2cm}

\[
M_{X}(t) = \mathbb{E}(e^{tX}) = G_{X}(e^t)
\]

\vspace{0.2cm}

con la sustituci\'on de $s = e^t$. En general:

\vspace{0.3cm}

\[
M_{X}(t) = \mathbb{E}(e^{tX}) = \begin{dcases}
\sum_{x}e^{tx}\mathbb{P}(X =x) & \ \ \text{si}\ \ X \ \ \text{es discreta}\\
\bigintsss_{-\infty}^{\infty}e^{tx}f_{X}(x)dx & \ \ \text{si}\ \ X \ \ \text{es continua}
\end{dcases}
\]

\vspace{0.2cm}

siempre que esta suma o integral converga absolutamente. En algunos casos, la existencia de $M_X(t)$ puede plantear un problema para valores $t$ distintos de cero.

\vspace{0.3cm}

\begin{ejemplo}
\normalfont Si $X$ tiene una distribuci\'on normal con media $0$ y varianza $1$, entonces

\vspace{0.2cm}


\begin{align*}
 M_{X}(t) &= \bigintsss_{-\infty}^{\infty}e^{tx}\dfrac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}x^2}dx \\
 &= e^{\frac{1}{2}t^2}\bigintsss_{-\infty}^{\infty}\dfrac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}(x -t)^2}dx \\
 & = e^{\frac{1}{2}t^2}
\end{align*}

\vspace{0.2cm}

desde que el integrando en la \'ultima integral es la funci\'on densidad de la distribuci\'on normal con media $t$ y varianza $1$, la integral tiene que ser $1$. El momento $M_{X}(t)$ existe para todo $t \in \mathbb{R}$.

\end{ejemplo}

\vspace{0.2cm}

\begin{ejemplo}
\normalfont Si $X$ tiene una distribuci\'on de Poisson con par\'ametro $\lambda$ y funci\'on de masa de probabilidad: 

\vspace{0.2cm}

\[
p_{X}(x) = \dfrac{\lambda^x e^{-\lambda}}{x!} \ \ , x =0, 1,2, \dots
\]

\vspace{0.2cm}

\[
M_X(t) = \displaystyle\sum_{x = 0}^{\infty}e^{tx}\dfrac{\lambda^{x}e^{-\lambda}}{x!}
\]

Si realizamos el cambio de variable $a = e^t\lambda$, entonces obtenemos $M_X(t) = e^{\lambda(e^t -1)}$.

\end{ejemplo}

\vspace{0.2cm}

\begin{ejemplo}
\normalfont Si $X$ tiene una distribuci\'on exponencial con param\'etro $\lambda$, entonces

\vspace{0.3cm}

\[
M_{X}(t) = \bigintsss_{0}^{\infty}e^{tx}\lambda e^{-\lambda x}dx = \begin{cases}
\dfrac{\lambda}{\lambda - t} & \ \text{si}\ \ t < \lambda\\
\infty & \ \ \text{si}\ \ t\geq \lambda.
\end{cases}
\]

\vspace{0.2cm}

as\'i existe $M_X(t)$ existe s\'olo para valores $t$ satisfaciendo $t < \lambda $.

\end{ejemplo}

\vspace{0.2cm}

\begin{ejemplo}
\normalfont Si $X$ tiene una distribuci\'on de Cauchy, entonces

\vspace{0.2cm}

\[
M_{X}(t) = \bigintsss_{-\infty}^{\infty}e^{tx}\dfrac{1}{\pi(1 + x^2)} dx = \begin{cases}
1 & \ \text{si}\ \ t = 0\\
\infty & \ \ \text{si}\ \ t\neq 0.
\end{cases}
\]

\vspace{0.2cm}

as\'i $M_X(t)$ existe s\'olo cuando $t = 0$.

\end{ejemplo}


\vspace{0.5cm}

La dificultad de la existencia de $\mathbb{E}(e^{tX})$ puede ser evitada usando la funci\'on de variable compleja $\phi_{X}(t) = \mathbb{E}(e^{itX})$ llamada \texttt{funci\'on caracter\'istica} de $X$ que existe para todo $t \in \mathbb{R}$.  Es importante decir que $\mathbb{E}(e^{tX})$ existe en alguna vecindad del origen y la raz\'on est\'a contenida en el teorema de unicidad de funciones generadores de momentos. Generalmente utilizaremos las funciones generadoras de momentos libremente, pero siempre sujetas a la asunci\'on impl\'icita de la existencia de una vecindad  del origen.

\vspace{0.5cm}

La  raz\'on del nombre de funci\'on generadora de momentos es la siguiente la expansi\'on que muestra que $M_{X}(t)$ es la funci\'on generadora exponencial de los momentos de $X$:


\vspace{0.2cm}

\begin{align*}
M_{X}(t) = \mathbb{E}(e^{tX}) = \mathbb{E}\Bigl(1 +tX + \dfrac{1}{2!}(tX)^2 + \cdots \Bigr) \\
 = 1 + t\mathbb{E}(X) + \dfrac{1}{2!}t^2\mathbb{E}(X^2) + \cdots
\end{align*}


\vspace{0.2cm}

\begin{teo}
\normalfont Si $M_{X}(t)$ existe en una vecindad de $0$, entonces para $k = 1, 2 \dots$

\vspace{0.2cm}

\[
\mathbb{E}(X^k) = M_{X}^{(k)}(0)
\]

\vspace{0.2cm}

la \texttt{k-\'esima} derivada de $M_X(t)$ evaluada en $t = 0$.

\end{teo}

\vspace{0.6cm}

Como se ha se\~nalado antes, mucho de la teoria se refiere a la suma de variables aleatorias, que puede ser dif\'icil en la pr\'actica calcular de la distribuci\'on de una suma a partir del conocimiento de las distribuciones de los sumandos y es aqu\'i donde las funciones generadoras de momentos  son muy \'utiles.

\vspace{0.3cm}

Consideremos primero  la funci\'on lineal $aX + b$ de la variable aleatoria $X$. Si $a ,b \in \mathbb{R}$


\vspace{0.2cm}

\begin{align*}
M_{aX +b}(t) = \mathbb{E}(e^{t(aX + b)}) = \mathbb{E}(e^{atX}e^{tb}) \\
= e^{tb}\mathbb{E}(e^{(at)X})
\end{align*}

\vspace{0.2cm}


lo que produce que $M_{aX + b}(t) = e^{tb}M_{X}(at)$.


\vspace{0.3cm}

\begin{teo}
\normalfont  Si $X$ e $Y$ son variables aleatorias independientes, entonces $X + Y$ tiene una funci\'on generadora de momentos:

\vspace{0.2cm}

\[
M_{X +Y}(t) = M_{X}(t)M_{Y}(t)
\]
\end{teo}

\vspace{0.3cm}

Tenemos por independencia y propiedad de la esperanza para la variable aleatoria $X$ y $Y$:

\vspace{0.2cm}

\begin{align*}
M_{X +Y}(t) = \mathbb{E}(e^{t(x + y)}) = \mathbb{E}(e^{tX}e^{tY}) \\
= \mathbb{E}(e^{tX})\mathbb{E}(e^{tY})
\end{align*}

\vspace{0.2cm}

En general la suma $S = X_1 + \cdots + X_n$ de $n$ variables aleatorias independientes tiene una funci\'on generadora de momentos de la forma:

\vspace{0.2cm}

\[
M_{S}(t) = M_{X_1}(t)\cdots M_{X_n}(t)
\]

\vspace{0.5cm}

\begin{teo}
\normalfont (Unicidad) Si la funci\'on generadora de momentos $M_{X}$ satisface $M_{X}(t) = \mathbb{E}(e^{tX}) < \infty$ para todo $t$ satisfaciendo $-\delta < t < \delta$ para alg\'un $\delta > 0$, entonces existe una \'unica distribuci\'on con  funci\'on generadora de momentos $M_{X}$. Adem\'as bajo esta condici\'on, se tiene que $\mathbb{E}(X^{k}) < \infty$ para $k = 1,2, \dots$ y

\vspace{0.2cm}

\[
M_{X}(t) = \displaystyle\sum_{k =0}^{\infty}\dfrac{1}{k!}t^k\mathbb{E}(X^k) \ \ \text{para} \ \ \vert t\vert < \delta.
\]
\end{teo}

\vspace{0.3cm}

\begin{ejemplo}
\normalfont Sean $X$ e $Y$ dos variables aleatorias independiendientes, teniendo $X$ una distribuci\'on normal con par\'ametros $\mu_1$ y $\sigma_1^2$ y $Y$  una distribuci\'on normal con par\'ametros $\mu_2$ y $\sigma_2^2$, veamos que propiedades tiene la suma $Z = X +Y$:

\vspace{0.2cm}

Sea $U$ una variable aleatoria que tiene una distribuci\'on normal con par\'ametros $\mu$ y $\sigma^2$. La funci\'on generadora de momentos de $U$ es:

\vspace{0.2cm}

\begin{align*}
M_U(t) &= \bigintsss_{-\infty}^{\infty}e^{tu}\dfrac{1}{\sqrt{2\pi\sigma^2}}\exp{\Bigl(-\dfrac{1}{2\sigma^2}(u -\mu)^2}\Bigr)du \\
=& e^{\mu t}\bigintsss_{-\infty}^{\infty}e^{x\sigma t}\dfrac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}x^2}dx \ \ \ \text{sustituyendo} \ \ \ x = \dfrac{u -\mu}{\sigma}\\
=& \exp(\mu t + \frac{1}{2}\sigma^2t^2)
\end{align*}

\vspace{0.2cm}

Por el teorema de unicidad:

\vspace{0.2cm}

\begin{align*}
M_{Z}(t) &= M_{X}(t)M_{Y}(t) \\
 &= \exp{(\mu_1 t + \frac{1}{2}\sigma_1^2t^2)}\exp{(\mu_2 t + \frac{1}{2}\sigma_2^2t^2)} \\
 &=\exp\Bigl[ (\mu_1 + \mu_2)t + \frac{1}{2}(\sigma_1^2 + \sigma_2^2)t^2\Bigr]
\end{align*}

\vspace{0.3cm}

Luego se deduce que es la funci\'on generadora de momentos  de la distribuci\'on normal con par\'ametros $\mu_1 + \mu_2$ y $\sigma_1^2 + \sigma_2^2$. As\'i $Z$ tiene esta distribuci\'on  apelando al teorema de unicidad.

\end{ejemplo}
\vspace{0.5cm}


\subsection{Transformada de Laplace}

\vspace{0.2cm}

Una funci\'on que est\'a estrechamente relacionada con la funci\'on generadora de momentos es la transformada de Laplace. Para una funci\'on real valorada $h(t), t \geq 0$, la transformada de Laplace, denotada por $h^{*}(s)$, es definida como:

\[
h^{*}(s) = \bigints_{0}^{\infty}e^{-sx}h(x)dx
\]

\vspace{0.2cm}

Si $h(t), t \geq 0$, es una funci\'on que corresponde a una funci\'on de densidad de una variable aleatoria no negativa $X$ entonces por la definici\'on de una funci\'on generadora de momentos,  la transformada de Laplace de la funci\'on $h^{*}(s)$ es igual a la funci\'on generadora de momentos de $X$ evaluado en $-s$, es decir:

\vspace{0.2cm}

\begin{equation}
h^{*}(s) = M_{X}(-s).
\end{equation}

\vspace{0.2cm}

Para preservar la sem\'antica asociada con variables aleatorias utilizamos la notaci\'on $\mathcal{L}_X$  cuando se trata de la transformada de Laplace de una variable aleatoria $X$:

\vspace{0.2cm}

\[
\mathcal{L}_X(s) = \bigints_{0}^{\infty}e^{-sx}f_X(x)dx.
\]

\vspace{0.2cm}

Una interpretaci\'on de la transformada para el caso de una variable aleatoria es que es la esperanza de $\exp[-sX]$ y  por tanto, puede escribirse como:

\[
\mathcal{L}_X(s) = \mathbb{E}[e^{-sX}].
\]


\vspace{0.2cm}

La transformada de Laplace es generalmente m\'as \'util que la funci\'on generadora de momentos, ya que puede aplicarse a funciones que no son densidades. La transformada de Laplace es la contrapartida continua de la funci\'on de generadora ordinaria que se vi\'o anteriormente. Para ver la relaci\'on entre estas dos funciones, supongamos que la funci\'on  $h(t)$ toma valores no nulos para el entero $t$, es decir, que $h(t)=h_i$, para $t=i$ e $i = 0,1, \dots$  y sea $H(x)$ la funci\'on generadora:

\[
H(x) = \sum_{i = 0}^{\infty}h_ix^i.
\]

\vspace{0.2cm}

Entonces, se sigue que:

\vspace{0.2cm}

\begin{equation}
h^{*}(s) = \bigints_{0}^{\infty}e^{-sx}h(x) dx = \sum_{j = 0}^{\infty}e^{-sj}h_j = H(e^{-s}).
\end{equation}

\vspace{0.2cm}

Y as\'i la transformaci\'on de Laplace es igual a la funci\'on generadora evaluado en $e^{-s}$. Si $h_i$ representa, a funci\'on de densidad de probabilidad, entonces las ecuaciones (3) y (4) muestran que simplemente hemos establecido una forma diferente de relaci\'on entre las funciones generadoras de momentos y sus correspondientes funciones generadoras de probabilidad para variables aleatorias discretas.

\vspace{0.2cm}

Dado que las transformadas de Laplace son equivalentes a funciones generadoras de momentos cuando la funci\'on subyacente es una densidad, se sigue que ellas satisfacen propiedades an\'alogas a las que se dan para las funciones generadoras de momentos. Por ejemplo, se cumple lo siguiente:

\[
\mathcal{L}_{X}^{(n)}(0) = (-1)^n\mathbb{E}(X^n).
\]

\vspace{0.2cm}

La transformada de Laplace se usa frecuentemente cuando se forma la \underline{convoluci\'on de dos variables aleatorias}. Para derivar la transformada de Laplace para una convoluci\'on, se $h(t)$  la funci\'on  densidad que resulta cuando convolucionamos dos funciones de densidad continuas $f$ y $g$. El operador de convoluci\'on para funciones de densidad se define como:

\[
h(t) = \bigints_{0}^{t}f(x -t)g(x)dx.
\]

\vspace{0.2cm}

De manera equivalente, escribimos la notaci\'on de convoluci\'on como:

\vspace{0.2cm}

\[
h(t) = f \odot g(t).
\]

\vspace{0.2cm}

Para calcula $h^{*}(s)$ escribimos:

\begin{align*}
h^{*}(s)& = \bigints_{0}^{\infty}e^{-st}h(t)dt \\
        & = \bigints_{0}^{\infty}e^{-st}dt\bigints_{0}^{t}f(t -x)g(x)dx \\
        & =  \bigints_{0}^{\infty}e^{-sx}g(x)dx \bigints_{0}^{\infty}e^{-s(t -x)}f(t -x)dx \\
        & = g^{*}(s)f^{*}(s).
\end{align*}

 \end{document}
