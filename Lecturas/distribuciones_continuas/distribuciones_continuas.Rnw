\documentclass{article}
\usepackage[sc]{mathpazo}
\usepackage[T1]{fontenc}
\usepackage{bigints}
\usepackage[latin1]{inputenc}
\usepackage{geometry}
\usepackage{amsmath} 
\usepackage{mathtools}
%\usepackage[spanish]{babel}
\usepackage{latexsym}
\geometry{verbose,tmargin=1.5cm,bmargin=2.5cm,lmargin=2.5cm,rmargin=2.5cm}
\setcounter{secnumdepth}{2}
\setcounter{tocdepth}{2}
\usepackage{url}
\usepackage{graphicx}
\newcommand{\R}{\mathbb{R}}
\newcommand{\bb}[1]{\textbf{#1}}
\DeclareMathOperator{\rank}{\textbf{rango}}
\DeclareMathOperator{\proy}{\textbf{proy}}
\DeclareMathOperator{\nulll}{\textbf{nul}}
\DeclareMathOperator{\diag}{\textbf{diag}}
\DeclareMathOperator{\col}{\textbf{col}}
\DeclareMathOperator{\fila}{\textbf{fila}}
\DeclareMathOperator{\dimm}{dim}
\DeclareMathOperator{\Traz}{Tr}
%\theoremstyle{definition}
\everymath{\displaystyle}
\newtheorem{ejemplo}{{Ejemplo }}[section]
\newtheorem{defi}{{Definici\'on}}[section]
\newtheorem{pros}{{Proposici\'on}}[section]
\newtheorem{cor}{{Corolario}}[section]
\usepackage[unicode=true,pdfusetitle,
 bookmarks=true,bookmarksnumbered=true,bookmarksopen=true,bookmarksopenlevel=2,
 breaklinks=false,pdfborder={0 0 1},backref=false,colorlinks=false]
 {hyperref}
\hypersetup{
 pdfstartview={XYZ null null 1}}
\begin{document}

<<setup, include=FALSE, cache=FALSE>>=
library(knitr)
library(ggplot2)
library(grid)
#library(animation)
# set global chunk options
opts_chunk$set(fig.path='figure/minimal-', fig.align='center', fig.show='hold')
options(formatR.arrow=TRUE,width=90)
@

%\title{\underline{\textbf{Notas de mat\'ematica}}}
%\date{}
%\maketitle
\hspace*{0.5\linewidth}
\begin{minipage}{0.6\linewidth}
Curso: C\'alculo de probabilidades CM-1H2\par
Distribuciones importantes \par
\end{minipage}

\vspace {0.5cm}

\section{Distribuciones continuas }
 
\vspace{0.2cm} 

Algunas de la m\'as importantes variables aleatorias continuas, se listan a continuaci\'on:

\vspace{0.3cm}

\subsection{La familia Normal}
 
 De acuerdo con el teorema del l\'imite central, la distribuci\'on limitante de la suma estandarizada un gran n\'umero de variables aleatorias independientes,es la distribuci\'on normal.
 
 \vspace{0.2cm}
 
 
 La densidad de $N(\mu, \sigma^2)$ es:
 
 \[
 f_{\mu, \sigma^2}(x) = \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x -\mu)^2}{2\sigma^2}}
 \]
 
 La media de una variable aleatoria  $N(\mu, \sigma^2)$ es:
 
 \[
 \mathbb{E}(X) = \mu
 \]
 
 y la varianza es:
 
 \[
 \text{Var}(X) = \sigma^2.
 \]

\vspace{0.2cm}

\begin{figure}[ht]
\centering
\includegraphics[scale=.5]{p3.png}
\end{figure}

\vspace{0.2cm}

\begin{figure}[ht]
\centering
\includegraphics[scale=.5]{p4.png}
\end{figure}

\vspace{0.2cm}

La \texttt{distribuci\'on normal est\'andar} tiene media $\mu = 0$ y $\sigma^2 = 1$, as\'i la funci\'on densidad:

\[
f(x ) = \frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}.
\]

La media es: $\mathbb{E}(X) = 0$ y la varianza $\text{Var}(X) = 1$.

\vspace{0.4cm}

El CDF de esta variable aleatoria, se denota como $\Phi$ y se define como:

\[
\Phi(t) = \frac{1}{\sqrt{2\pi}}\bigints_{-\infty}^{t}e^{-\frac{x^2}{2}} dx.
\]

\vspace{0.3cm}

Si $Z$ es una variable normal est\'andar, entonces:

\[
\sigma Z + \mu  \sim N(\mu, \sigma^2)
\]

y si 

\[
X \sim N(\mu, \sigma^2), \ \ \text{entonces}\  \frac{x - \mu}{\sigma} \sim N(0,1)
\]

\vspace{0.2cm}

Si $X$ y $Z$ son normales e independientes, entonces $aX + bZ$ es tambi\'en normal.


\begin{figure}[ht]
\centering
\includegraphics[scale=.5]{p1.png}
\end{figure}

\vspace{0.2cm}

\begin{figure}[ht]
\centering
\includegraphics[scale=.5]{p2.png}
\end{figure}

\vspace{0.2cm}


\subsection{Familia exponencial}

La familia exponencial se usa para modelar tiempos de espera. Una variable aleatoria exponencial \texttt{Exponencial ($\lambda$)} tiene una densidad:

\[
f(t) = \lambda e^{-\lambda t}
\]

y un CDF: 

\[
F(t) = 1 -e^{-\lambda t}.
\]

\vspace{0.2cm}

Si definimos la funci\'on (survival function):

\[
G(t) = 1 -F(t) = e^{-\lambda t}.
\]

Entonces la tasa de Hazard $f(t)/G(t)$ es igual a $\lambda$.

\vspace{0.2cm}

La media de la \texttt{Exponencial ($\lambda$)} es:

\[
\mathbb{E}(T) = \frac{1}{\lambda}
\]

y la varianza es:

\[
\text{Var}(T) = \frac{1}{\lambda^2}.
\]

\vspace{0.2cm}

La familia xxponencial tiene la \textbf{propiedad de Markov} o \texttt{memoryless}:

\[
\mathbb{P}(T < t +s | T > t) = \mathbb{P}(T > s).
\]

\vspace{0.2cm}


\begin{figure}[ht]
\centering
\includegraphics[scale=.5]{p5.png}
\end{figure}

\vspace{0.2cm}

\begin{figure}[ht]
\centering
\includegraphics[scale=.5]{p6.png}
\end{figure}


\subsection{Distribuci\'on uniforme}

Una variable aleatoria continua $X$ que es igualmente probable que tome  cualquier valor en un rango de valores $(a, b)$, con $a <b$, da lugar a la distribuci\'on uniforme. Dicha distribuci\'on est\'a uniformemente distribuida en su rango. Algunas veces se denota como $X \sim \text{Uniforme}(a,b)$.

\vspace{0.3cm}

La funci\'on densidad de la distribuci\'on es:

\[
f(x) = \begin{cases}
\frac{1}{b -a} & x \in [a,b] \\
0 & \text{en otros casos}.
\end{cases}
\]

\vspace{0.2cm}

El CDF es:

\vspace{0.2cm}


\[
F(x) = \begin{cases}
\frac{x- a}{b -a} & x \in [a,b] \\
0 & \text{en otros casos}.
\end{cases}
\]

\vspace{0.2cm}

La media y la varianza de la variable aleatoria uniforme es:

\[
\mathbb{E}(X) = \frac{a + b}{2}, \ \ \ \text{Var}(X) = \frac{(b -a)^2}{12}.
\]


\vspace{0.2cm}

\begin{figure}[ht]
\centering
\includegraphics[scale=.6]{p13.png}
\end{figure}

\vspace{0.2cm}

\begin{figure}[ht]
\centering
\includegraphics[scale=.6]{p14.png}
\end{figure}

\subsection{Distribuci\'on de Cauchy}

Si $X$ e $Y$ son normales est\'andar independientes, entonces $Y/X$ tiene una distribuci\'on de Cauchy. La distribuci\'on de Cauchy, es la distribuci\'on de la tangente de un \'angulo aleatoriamente seleccionado desde $[-\pi, \pi]$.

\vspace{0.2cm}

La densidad es:

\[
f(x) = \frac{1}{\pi(1 + x^2)},
\]


\vspace{0.3cm}

y el CDF:

\vspace{0.2cm}


\[
F(t) = \frac{1}{\pi}\arctan(t) + \frac{1}{2}.
\]

\vspace{0.2cm}


\begin{align*}
\int_{-\infty}^{\infty}f(x)dx = \frac{1}{\pi}\int_{-\infty}^{\infty}\frac{dx}{(1 + x^2)} =  \frac{1}{\pi}\int_{-\infty}^{\infty}\frac{d\tan^{-1}(x)}{dx}\\ =
\frac{1}{\pi}[\tan^{-1}(\infty) - \tan^{-1}(-\infty)] 
&=  \frac{1}{\pi}[\frac{\pi}{2} - (-\frac{\pi}{2})] = 1.
\end{align*}

\vspace{0.2cm}

La esperanza de una distribuci\'on de Cauchy no existe :


\vspace{0.3cm}

\begin{align*}
\int_{0}^{t}\frac{x }{\pi(1 + x^2)}dx = \frac{\ln (1 + t^2)}{2 \pi} \rightarrow  \infty\ \ \text{si}\ \ t \rightarrow \infty. \\
\int_{-t}^{0}\frac{x }{\pi(1 + x^2)}dx = \frac{-\ln (1 + t^2)}{2 \pi} \rightarrow  -\infty\ \ \text{si}\ \ t \rightarrow \infty.
\end{align*}


\vspace{0.2cm}

As\'i

\[
\int_{-\infty}^{\infty}\frac{x }{\pi(1 + x^2)}dx \ \ \ \text{es divergente y sin sentido}.
\]

\vspace{0.2cm}

\begin{figure}[ht]
\centering
\includegraphics[scale=.6]{p11.png}
\end{figure}

\vspace{0.2cm}

\begin{figure}[ht]
\centering
\includegraphics[scale=.6]{p12.png}
\end{figure}


\subsection{Distribuci\'on beta}

$X$ tiene una distribuci\'on $\mbox{Beta}$ con par\'ametros $\alpha > 0$  y $\beta > 0$, denotado por $X \sim \mbox{Beta}(\alpha, \beta)$, si


\vspace{0.2cm}

\[
f(x) = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)}x^{\alpha -1}(1 -x)^{\beta -1}, \ \ 0< x < 1.
\]

\vspace{0.2cm}

La media y la varianza de una distribuci\'on Beta es:

\[
\mathbb{E}(X) = \frac{\alpha }{\alpha + \beta}\ \ \ \text{Var}(X) = \frac{\alpha \beta}{(\alpha + \beta)^2(1 + \alpha + \beta)}.
\]

\vspace{0.2cm}

\begin{figure}[ht]
\centering
\includegraphics[scale=.5]{p15.png}
\end{figure}

\vspace{0.2cm}

\begin{figure}[ht]
\centering
\includegraphics[scale=.5]{p16.png}
\end{figure}


\vspace{0.2cm}

\begin{figure}[ht]
\centering
\includegraphics[scale=.5]{p17.png}
\end{figure}


\subsection{La distribuci\'on $\chi^2(n)$}

Sea $Z_1, \dots, Z_n$ variables aleatorias normales est\'andar. La distribuci\'on Chi-cuadrado con $n$ grados de libertad, $\chi^2(n)$ es la distribuci\'on de:

\[
R_n^2 =Z_1^2 +\cdots + Z_n^2.
\]

\vspace{0.2cm}

La funci\'on densidad de esta distribuci\'on, est\'a dado por:

\[
f_{R_n^2}(t) = \frac{1}{2^{n/2}\Gamma(n/2)}t^{(n/2) -1}e^{-t/2}, \ \ (t > 0), 
\]

y escribimos:

\[
R_n^2 \sim \chi^2(n).
\]

\vspace{0.2cm}

La media y varianza son caracterizadas por:

\[
\mathbb{E}(R_n^2) =n, \ \ \text{Var}(R_n^2) = 2n.
\]

\vspace{0.2cm}

\begin{figure}[ht]
\centering
\includegraphics[scale=.5]{p18.png}
\end{figure}

\vspace{0.3cm}

Se sigue de la definici\'on  que si $X$ e $Y$ son independientes y $X \sim \chi^2(n)$ y $Y \sim \chi^2(m)$, entonces:

\[
(X + Y) \sim \chi^2(n + m).
\]

\vspace{0.2cm}

\begin{figure}[ht]
\centering
\includegraphics[scale=.5]{p19.png}
\end{figure}

\vspace{0.3cm}

Esta es una distribuci\'on importante en estad\'istica inferencial y es la base de la prueba de ajustes chi-cuadrado y el m\'etodo de estimaci\'on de  m\'inimos de  chi-cuadrado.

\vspace{0.2cm}

Si hay $m$ resultados posibles de un experimento y sea $p_i$ la probabilidad de que el resultado $i$ ocurre. Si el experimento se repite independientemente $N$ veces, sea $N_i$ el n\'umero de veces que se observa el resultado $i$, por lo que $N = N_1 + \cdots + N_m$. Entonces la estad\'istica de  chi-cuadrado es:

\vspace{0.2cm}

\[
\sum_{i =1}^{n}\frac{(N_i - Np_i)^2}{Np_i},
\]

\vspace{0.2cm}

converge en distribuci\'on cuando $N \rightarrow$ a  una distribuci\'on $\chi^2(m -1)$ con $ m -1$ grados de libertad.


\subsection{Familia gamma}

La familia  de distribuciones \texttt{Gamma(r, $\lambda$)} es vers\'atil. La distribuci\'on de la suma de $r$  variables aleatorias \texttt{Exponencial($\lambda$)} independientes, la distribuci\'on del $r$- \'esimo tiempo de llegada  en un proceso de Poisson con tasa de llegada $\lambda$ es la distribuci\'on  \texttt{Gamma(r, $\lambda$)}.

\vspace{0.2cm}

La distribuci\'on de la suma de cuadrados de $n$ distribuciones  normales est\'andar independientes, la distribuci\'on $\chi^2(n)$ es la distribuci\'on  \texttt{Gamma(1/2, 1/2)}.

\vspace{0.3cm}

La distribuci\'on  \texttt{Gamma(r, $\lambda$)}  en general ($r > 0, \lambda > 0$), tiene una densidad dado por:

\[
f(t) = \frac{\lambda^r}{\Gamma(r)}t^{r -1}e^{-\lambda t}\ \ (t > 0).
\]

\vspace{0.2cm}

La media y la varianza de un variable aleatoria \texttt{Gamma(r, $\lambda$)} es dado por:

\[
\mathbb{E}(X) = \frac{r}{\lambda}\ \ \ \text{Var}(X) = \frac{r}{\lambda^2}.
\]

\vspace{0.2cm}


\begin{figure}[ht]
\centering
\includegraphics[scale=.4]{p7.png}
\end{figure}

\vspace{0.2cm}

\begin{figure}[ht]
\centering
\includegraphics[scale=.4]{p9.png}
\end{figure}

\section{Relaciones entre  distribuciones}

El gr\'afico siguiente est\'a adaptado desde el \href{https://www.johndcook.com/blog/}{blog}  de Jhon Cook, del gr\'afico publicado originalmente por Lawrence Leemis en 1986 (Relationships Among Common Univariate Distributions, American Statistician 40: 143-146). Leemis public\'o un gr\'afico m\'as grande en 2008, que esta disponible en l\'inea. Lo puedes descargar \href{http://www.math.wm.edu/~leemis/2008amstat.pdf}{aqu\'i}.

\begin{figure}[htb]
\centering
\includegraphics[scale=.5]{distribuciones.png}
\end{figure}

\end{document}
